
OSI Model
The Open Systems Interconnection model (OSI Model) is a conceptual model that characterizes and standardizes the communication functions of a telecommunication or computing system without regard of their underlying internal structure and technology. Its goal is the interoperability of diverse communication systems with standard protocols. The model partitions a communication system into abstraction layers. The original version of the model defined seven layers.

A layer serves the layer above it and is served by the layer below it. For example, a layer that provides error-free communications across a network provides the path needed by applications above it, while it calls the next lower layer to send and receive packets that comprise the contents of that path. Two instances at the same layer are visualized as connected by a horizontal connection in that layer.

The model is a product of the Open Systems Interconnection project at the International Organization for Standardization (ISO), maintained by the identification ISO/IEC 7498-1.

In the late 1970s, two projects began independently, with the same goal: to define a unifying standard for the architecture of networking systems.[citation needed] One was administered by the International Organization for Standardization (ISO), while the other was undertaken by the International Telegraph and Telephone Consultative Committee, or CCITT (the abbreviation is from the French version of the name). These two international standards bodies each developed a document that defined similar networking models.

In 1983, these two documents were merged to form a standard called The Basic Reference Model for Open Systems Interconnection. The standard is usually referred to as the Open Systems Interconnection Reference Model, the OSI Reference Model, or simply the OSI model. It was published in 1984 by both the ISO, as standard ISO 7498, and the renamed CCITT (now called the Telecommunications Standardization Sector of the International Telecommunication Union or ITU-T) as standard X.200.

OSI had two major components, an abstract model of networking, called the Basic Reference Model or seven-layer model, and a set of specific protocols.

The concept of a seven-layer model was provided by the work of Charles Bachman at Honeywell Information Services. Various aspects of OSI design evolved from experiences with the ARPANET, NPLNET, EIN, CYCLADES network and the work in IFIP WG6.1. The new design was documented in ISO 7498 and its various addenda. In this model, a networking system was divided into layers. Within each layer, one or more entities implement its functionality. Each entity interacted directly only with the layer immediately beneath it, and provided facilities for use by the layer above it.

Protocols enable an entity in one host to interact with a corresponding entity at the same layer in another host. Service definitions abstractly described the functionality provided to an (N)-layer by an (N-1) layer, where N was one of the seven layers of protocols operating in the local host.

Description of OSI layers[edit]
The recommendation X.200 describes seven layers, labeled 1 to 7. Layer 1 is the lowest layer in this model.

OSI model
7  Application Layer				High-level APIs, including resource sharing, remote file access, directory services and virtual terminals
6  Presentation Layer				Translation of data between a networking service and an application; including character encoding, data compression and encryption/decryption
5  Session Layer					Managing communication sessions, i.e. continuous exchange of information in the form of multiple back-and-forth transmissions between two nodes
4  Transport Layer					Reliable transmission of data segments between points on a network, including segmentation, acknowledgement and multiplexing
3  Network Layer					Structuring and managing a multi-node network, including addressing, routing and traffic control
2  Data link Layer					Reliable transmission of data frames between two nodes connected by a physical layer
1  Physical Layer					Transmission and reception of raw bit streams over a physical medium

Layer 1: Physical Layer[edit]
The physical layer has the following major functions:

It defines the electrical and physical specifications of the data connection. It defines the relationship between a device and a physical transmission medium (e.g., a copper or fiber optical cable). This includes the layout of pins, voltages, line impedance, cable specifications, signal timing, hubs, repeaters, network adapters, host bus adapters (HBA used in storage area networks) and more.
It defines the protocol to establish and terminate a connection between two directly connected nodes over a communications medium.
It may define the protocol for flow control.
It defines transmission mode i.e. simplex, half duplex, full duplex.
It defines topology.
The physical layer of Parallel SCSI operates in this layer, as do the physical layers of Ethernet and other local-area networks, such as Token Ring, FDDI, ITU-T G.hn, and IEEE 802.11 (Wi-Fi), as well as personal area networks such as Bluetooth and IEEE 802.15.4.

Layer 2: Data Link Layer[edit]
The data link layer provides node-to-node data transfer -- a reliable link between two directly connected nodes, by detecting and possibly correcting errors that may occur in the physical layer. The data link layer is divided into two sublayers:

Media Access Control (MAC) layer - responsible for controlling how devices in a network gain access to data and permission to transmit it.
Logical Link Control (LLC) layer - controls error checking and packet synchronization.
The Point-to-Point Protocol (PPP) is an example of a data link layer in the TCP/IP protocol stack.

The ITU-T G.hn standard, which provides high-speed local area networking over existing wires (power lines, phone lines and coaxial cables), includes a complete data link layer that provides both error correction and flow control by means of a selective-repeat sliding-window protocol.

Layer 3: Network Layer[edit]
The network layer provides the functional and procedural means of transferring variable length data sequences (called datagrams) from one node to another connected to the same network. It translates logical network address into physical machine address. A network is a medium to which many nodes can be connected, on which every node has an address and which permits nodes connected to it to transfer messages to other nodes connected to it by merely providing the content of a message and the address of the destination node and letting the network find the way to deliver ("route") the message to the destination node. In addition to message routing, the network may (or may not) implement message delivery by splitting the message into several fragments, delivering each fragment by a separate route and reassembling the fragments, report delivery errors, etc.

Datagram delivery at the network layer is not guaranteed to be reliable.

A number of layer-management protocols, a function defined in the management annex, ISO 7498/4, belong to the network layer. These include routing protocols, multicast group management, network-layer information and error, and network-layer address assignment. It is the function of the payload that makes these belong to the network layer, not the protocol that carries them.

Layer 4: Transport Layer[edit]
The transport layer provides the functional and procedural means of transferring variable-length data sequences from a source to a destination host via one or more networks, while maintaining the quality of service functions.

An example of a transport-layer protocol in the standard Internet stack is Transmission Control Protocol (TCP), usually built on top of the Internet Protocol (IP).

The transport layer controls the reliability of a given link through flow control, segmentation/desegmentation, and error control. Some protocols are state- and connection-oriented. This means that the transport layer can keep track of the segments and retransmit those that fail. The transport layer also provides the acknowledgement of the successful data transmission and sends the next data if no errors occurred. The transport layer creates packets out of the message received from the application layer. Packetizing is a process of dividing the long message into smaller messages.

OSI defines five classes of connection-mode transport protocols ranging from class 0 (which is also known as TP0 and provides the fewest features) to class 4 (TP4, designed for less reliable networks, similar to the Internet). Class 0 contains no error recovery, and was designed for use on network layers that provide error-free connections. Class 4 is closest to TCP, although TCP contains functions, such as the graceful close, which OSI assigns to the session layer.

Layer 5: Session Layer[edit]
The session layer controls the dialogues (connections) between computers. It establishes, manages and terminates the connections between the local and remote application. It provides for full-duplex, half-duplex, or simplex operation, and establishes checkpointing, adjournment, termination, and restart procedures. The OSI model made this layer responsible for graceful close of sessions, which is a property of the Transmission Control Protocol, and also for session checkpointing and recovery, which is not usually used in the Internet Protocol Suite. The session layer is commonly implemented explicitly in application environments that use remote procedure calls.

Layer 6: Presentation Layer[edit]
The presentation layer establishes context between application-layer entities, in which the application-layer entities may use different syntax and semantics if the presentation service provides a big mapping between them. If a mapping is available, presentation service data units are encapsulated into session protocol data units, and passed down the protocol stack.

This layer provides independence from data representation (e.g., encryption) by translating between application and network formats. The presentation layer transforms data into the form that the application accepts. This layer formats and encrypts data to be sent across a network. It is sometimes called the syntax layer.[6]

The original presentation structure used the Basic Encoding Rules of Abstract Syntax Notation One (ASN.1), with capabilities such as converting an EBCDIC-coded text file to an ASCII-coded file, or serialization of objects and other data structures from and to XML.

Layer 7: Application Layer[edit]
The application layer is the OSI layer closest to the end user, which means both the OSI application layer and the user interact directly with the software application. This layer interacts with software applications that implement a communicating component. Such application programs fall outside the scope of the OSI model. Application-layer functions typically include identifying communication partners, determining resource availability, and synchronizing communication. When identifying communication partners, the application layer determines the identity and availability of communication partners for an application with data to transmit. When determining resource availability, the application layer must decide whether sufficient network or the requested communication exists. In synchronizing communication, all communication between applications requires cooperation that is managed by the application layer. 

===
Open Systems Interconnection (OSI) is an effort to standardize computer networking that was started in 1977 by the International Organization for Standardization (ISO), along with the ITU-T.[1]

Prior to OSI, networking was largely either government-sponsored (ARPANET in the US, CYCLADES in France) or vendor-developed and proprietary standards (such as the System network architecture (SNA) of IBM and DECnet of Digital Equipment Corporation). A Experimental Packet Switched system in the UK circa 1973, also identified the need to define higher level protocols. The NCC (UK) publication 'Why Distributed Computing' which came from considerable research into future configurations for computer systems, resulted in the UK presenting the case for an international standards committee to cover this area at the ISO meeting in Sydney in March 1977. OSI was hence an industry effort, attempting to get industry participants to agree on common network standards to provide multi-vendor interoperability. It was common for large networks to support multiple network protocol suites, with many devices unable to interoperate with other devices because of a lack of common protocols. However, while OSI developed its networking standards, TCP/IP came into widespread use on multivendor networks for internetworking, while on the local network level both Ethernet and token ring gained prominence.

The OSI reference model was a major advance in the teaching of network concepts. It promoted the idea of a consistent model of protocol layers, defining interoperability between network devices and software. The OSI model was defined in raw form in Washington, DC in February 1978 by Hubert Zimmermann of France and the refined standard was published by the ISO in 1984.
===
A VMScluster is a computer cluster involving a group of computers running the OpenVMS operating system. Whereas tightly coupled multiprocessor systems run a single copy of the operating system, a VMScluster is loosely coupled: each machine runs its own copy of OpenVMS, but the disk storage, lock manager, and security domain are all cluster-wide. Machines can join or leave a VMScluster without affecting the rest of the cluster. For enhanced availability, VMSclusters support the use of dual-ported disks connected to two machines or storage controllers simultaneously. With OpenVMS now ported to Alpha and IA-64 machines, the facility originally named VAXclustering was renamed to VMSclustering.

Digital Equipment Corporation first announced VAXclusters in May 1983. At this stage, clustering required specialised communications hardware, as well as some major changes to low-level subsystems in VMS. The software and hardware were designed jointly.

At the center of each cluster was a star coupler, to which every node (computer) and data storage device in the cluster was connected by one or two pairs of CI cables. ("CI" stands for Computer Interconnect.) Each pair of cables had a transmission rate of 70 megabits per second, a high speed for that era. Using two pairs gave an aggregate transmission rate of 140 megabits per second, with redundancy in case one cable failed; the star couplers also had redundant wiring for better availability.

Each CI cable connected to its computer via a CI Port, which could send and receive packets without any CPU involvement. To send a packet, a CPU had only to create a small data structure in memory and append it to a "send" queue; similarly, the CI Port would append each incoming message to a "receive" queue. Tests showed that a VAX-11/780 could send and receive 3000 messages per second, even though it was nominally a 1-MIPS machine. The closely related Mass Storage Control Protocol (MSCP) allowed similarly high performance from the mass storage subsystem. In addition, MSCP packets were very easily transported over the CI allowing remote access to storage devices.

VAXclustering was the first clustering system to achieve commercial success, and was a major selling point for VAX systems.
===
DECnet is a suite of network protocols created by Digital Equipment Corporation, originally released in 1975 in order to connect two PDP-11 minicomputers. It evolved into one of the first peer-to-peer network architectures, thus transforming DEC into a networking powerhouse in the 1980s. Initially built with three layers, it later (1982) evolved into a seven-layer OSI-compliant networking protocol.

DECnet was built right into the DEC flagship operating system VMS since its inception. Later Digital ported it to Ultrix, as well as Apple Macintosh and IBM PC running variants of DOS and Microsoft Windows under the name DEC Pathworks, allowing these systems to connect to DECnet networks of VAX machines as terminal nodes. More recently, an open-source version has been developed for GNU/Linux.[1] DECnet code in the Linux kernel was marked as orphaned on February 18, 2010.[2]

===
Local Area Transport (LAT) is a non-routable (Data Link Layer) networking technology developed by Digital Equipment Corporation[1] to provide connection between the DECserver 90, 100, 200, 300, 500, 700 and DECserver 900 terminal servers and Digital's VAX and Alpha and MIPS host computers via Ethernet, giving communication between those hosts and serial devices such as video terminals and printers. The protocol itself was designed in such a manner as to maximize packet efficiency over Ethernet by bundling multiple characters from multiple ports into a single packet for Ethernet transport (Mann, US 4823122). Over time, other host implementations of the LAT protocol appeared allowing communications to a wide range of Unix and other non-Digital operating systems using the LAT protocol.

In 1984, the first implementation of the LAT protocol connected a terminal server to a VMS VAX-Cluster in Spit Brook Road, Nashua, NH. By "virtualizing" the terminal port at the host end, a very large number of plug-and-play VT100-class terminals could connect to each host computer system. Additionally, a single physical terminal could connect via multiple sessions to multiple hosts simultaneously. Future generations of terminal servers included both LAT and TELNET protocols, one of the earliest protocols created to run on a burgeoning TCP/IP based Internet. Additionally, the ability to create reverse direction pathways from users to non-traditional RS232 devices (i.e. UNIX Host TTYS1 operator ports) created an entirely new market for Terminal Servers, now known as console servers in the mid to late 1990s, year 2000 and beyond through today.

LAT and VMS drove the initial surge of adoption of "thick-wire" Ethernet by the computer industry. By 1986, terminal server networks accounted for 10% of Digital's $10 billion revenue. These early Ethernet LANs scaled using Ethernet bridges (another DEC invention) as well as DECnet routers. Subsequently, Cisco routers, which implemented TCP-IP and DECnet, emerged as a global connection between these packet-based Ethernet LANs.

Over time, when terminals became less popular, terminal emulators had a built-in LAT client.

===
The VT100 is a video terminal, introduced in August 1978 by Digital Equipment Corporation (DEC). It was one of the first terminals to support ANSI escape codes for cursor control and other tasks, and added a number of extended codes for special features like controlling the LED lamps on the keyboard. This led to rapid uptake of the ANSI standard, becoming the de facto standard for terminal emulators.

The VT100s, especially the VT102, was extremely successful in the market, and made DEC the leading terminal vendor for some time. The VT100 series was replaced by the VT200 series starting in 1983, which proved just as successful. Ultimately, over six million terminals in the VT series would be sold, based largely on the success of the VT100s.[1]

The VT100 was introduced in August 1978, replacing the VT50/VT52 family. Like the earlier models, it communicated with its host system over serial lines using the ASCII character set and control sequences. Unlike the VT52's custom control sequences, the VT100 used sequences standardized by ANSI. The VT100 was also the first Digital mass-market terminal to incorporate "graphic renditions" (blinking, bolding, reverse video, and underlining) as well as 80 or 132 column displays. The VT100 also introduced an additional character set containing various bits of graphics that allowed the drawing of on-screen forms. All setup of the VT100 was accomplished using interactive displays presented on the screen; the setup data was stored in non-volatile memory within the terminal.

The control sequences used by the VT100 family are based on the ANSI X3.64 standard, also known as ECMA-48 and ISO/IEC 6429. These are sometimes referred to as ANSI escape codes. The VT100 was not the first terminal to be based on X3.64—The Heath Company had a microprocessor-based video terminal, the Heathkit H-19 (H19), that implemented a subset of the standard proposed by ANSI in X3.64.[2] In addition, the VT100 provided backwards compatibility for VT52 users, with support for the VT52 control sequences.[3]

In 1983, the VT100 was replaced by the more-powerful VT200 series terminals such as the VT220.

The VT100 was the first of Digital's terminals to be based on an industry-standard microprocessor, the Intel 8080. Options could be added to the terminal to support an external printer, additional graphic renditions, and more memory. The later option, known as Advanced Video Option or AVO, allowed the terminal to support a full 24 lines of text in 132 column mode. The VT100 became a platform on which Digital constructed related products.

The VT101 and VT102 were cost-reduced, non-expandable follow-on versions. The VT101 was essentially a base-model VT100, while the VT102 came standard with the AVO and serial printer port options pre-installed. The VT105 contained a simple graphics subsystem known as waveform graphics which was mostly compatible with same system in the earlier VT55. This system allowed two mathematical functions to be drawn to the screen on top of the normal text display, allowing text and graphics to be mixed to produce charts and similar output.[4] The VT125 added an implementation of the byte-efficient Remote Graphic Instruction Set, ReGIS, which used custom ANSI codes to send the graphics commands to the terminal, rather than requiring the terminal to be set to a separate graphics mode like the VT105.

The VT100 form factor left significant room in the case for expansion, and DEC used this to produce several all-in-one stand-alone minicomputer systems. The VT103 included a cardcage and 4×4 (8-slot) Q-Bus backplane, sufficient to configure a small LSI-11 system within the case,[5]:pp65–72 and supported an optional dual TU58 DECtape II block addressable cartridge tape drive [5]:pp73–80 which behaves like a very slow disk drive. The VT180 (codenamed "Robin") added a single-board microcomputer using a Zilog Z80 to run CP/M. The VT278 (DECmate) added a PDP-8 processor, allowing the terminal to run Digital's WPS-8 word processing software.

===
ReGIS, short for Remote Graphic Instruction Set, was a vector graphics markup language developed by Digital Equipment Corporation (DEC) for later models of their famous VT series of computer terminals. ReGIS supported rudimentary vector graphics consisting of lines, circular arcs, and similar shapes. Terminals supporting ReGIS generally allowed graphics and text to be mixed on-screen, which made construction of graphs and charts relatively easy.

ReGIS was first introduced on the VT125 in July 1981, followed shortly thereafter by the VK100 "GIGI" which combined the VT125 display system with composite video output and a BASIC interpreter. Later versions of the VT series included ReGIS, often with color support as well. This included the VT240 and 241, VT330 and 340, and the VT525. ReGIS is also supported by a small number of terminal emulator systems.

ReGIS replaced an earlier system known as waveform graphics that had been introduced on the VT55 and later used on the VT105. DEC normally provided backward compatibility with their terminals, but in this case the waveform system was simply dropped when ReGIS was introduced.
===
The VT220 was an ANSI standard computer terminal introduced by Digital Equipment Corporation (DEC) in 1983.[1][2] The VT240 added monochrome ReGIS vector graphics support to the base model, while the VT241 did the same in color. The 200 series replaced the successful VT100 series, providing more functionality in a much smaller unit with a much smaller and lighter keyboard. Among its major upgrades was a number of international characters, as well as the ability to define new character sets.

The VT200 series was extremely successful in the market. Released at $795, the VT220 offered features, packaging and price that no other serial terminal could touch. In 1986, DEC shipped 165,000 units, giving them a 42% market share, double that of the closest competitor, Wyse. Competitors adapted by introducing similar models at lower price points, leading DEC to do the same by releasing the less-expensive $545 VT300 series in 1987. By that time, DEC had shipped over one million VT220s.[3]

The VT220 improved on the earlier VT100 series of terminals with a redesigned keyboard, much smaller physical packaging, and a much faster microprocessor. The VT220 was available with CRTs that used white, green, or amber phosphors.

The VT100s, like the VT50s before them, had been packaged in relatively large cases that provided room for expansion systems. The VT200s abandoned this concept, and wrapped the much smaller 1980s-era electronics tightly around the CRT. The result was a truncated pyramidal case with the apex at the back, only slightly larger than the CRT. This made it much easier to fit the terminal on a desk. An adjustable stand allowed the angle of the CRT to be adjusted up and down. Because it was lower than head height, the result was an especially ergonomic terminal.

The LK201 keyboard supplied with the VT220 was one of the first full-length, low-profile keyboards available; it was developed at DEC's Roxbury, Massachusetts facility. It was much smaller and lighter than the VT100s version, and connected to the terminal using a lighter and more flexible coiled cable and a telephone jack connector.

The VT200s were the last DEC terminals to provide a 20mA current loop serial interface, an older standard originally developed for the telegraph system. A standard 25-pin D-connector was also provided for RS-232. Only one of the two ports could be in use at a given time. Later DEC terminals would replace both of these with their proprietary Modified Modular Jack (MMJ) connectors.

The VT220 was designed to be compatible with the VT100, but added features to make it more suitable for an international market. This was accomplished by including a number of different character sets that could be selected among using a series of ANSI commands. The terminal shipped with a total of 288 characters in its ROM, each one formed from a 8 by 10 pixel glyph (the character grid was 10 by 10, DEC only used 8 by 10). The characters included the 96 printable ASCII characters, 67 Display Controls, 32 DEC Special Graphics, and an upside-down question mark, used to represent undefined characters.

The VT200s included the ability to make minor changes to the character set using the National Replacement Character Set (NRCS) concept. When operating on an 8-bit clean link up to 256 character codes were available, which included a full set of European characters. But when operating on a typical 7-bit link, only 128 were available, and only 96 of these produced display output as the rest were control characters. This was not enough characters to handle all European languages. Most terminals solved this by shipping multiple complete character sets in ROM, but there was a cost in doing so.

DEC's solution to this problem, NCRS, allowed individual characters glyphs in the base set of 96 7-bit characters to be swapped out. For instance, the British set replaced the US's hash character, # for the pound sign,£. The terminal included 14 such replacement sets.[4] This eliminated the need to ship 14 versions of the terminal, or to include 14 different 7-bit character sets in ROM.

Additionally, the VT200s allowed for another 96 characters in the Dynamically Redefined Character Set (DRCS), which could be downloaded from the host computer. Data for the glyphs was sent by encoding a set of six vertical pixels into a single character code, and then sending many of these Sixels to the terminal, which decoded them into the character set memory. In later models, the same sixel concept would be used to send bitmapped graphics as well. Character graphics were a common example of these downloaded sets.[5]

===
The VT320 was an ANSI standard computer terminal introduced by Digital Equipment Corporation (DEC) in 1987.[1] The VT320 was the text-only version, while the VT330 added monochrome ReGIS, Sixel and Tektronix 4010 graphics, and the VT340 added color. The VT320 was replaced by the VT420 in 1990, but the VT340 remained in production until all of these models were replaced by the VT500 series in 1994.

The 300 series replaced the earlier VT200 series, as a lower-cost system better able to compete with a number of VT220 clones that had entered the market. Foremost among these was the Wyse WT-60, introduced in 1986 with a form factor and feature set similar to the VT220, but including 4010 graphics and selling for only $699, compared to $795 for the base-model VT220 lacking graphics. The VT320 was introduced at $545, something of a surprise,[2] forcing Wyse to lower their prices to $599.

The VT300s introduced a number of new features compared to the VT200s. With the great increase in RAM available, the 300's added the ability to store several pages of data locally, as well as perform editing on that data entirely within the terminal. The user could scroll up and down among several pages, normally about three, perform edits, and then send all of the changes to the host in a single operation. This required compatible host-side software to work. That memory also meant all of the 300 series were able to store large numbers of sixel-based glyphs, allowing them to be used not only as a user-defined character set as in the earlier 200's, but also to produce full-screen bitmap graphics by storing a separate sixel for each location on the screen.

Finally, all members of the line could support two sessions, either using two MMJ ports available on some models, or in the case of the VT330 and VT340, using a single serial connection using a system known as TD/SMP on the server and SSU on the terminals. The TD/SMP protocol was never published, and only worked with DEC's own terminal servers. Using either system, the terminal could display the two sessions "stacked" and switch between them, or by splitting the screen vertically to show them one above the other, or horizontally side-by-side. The serial ports could run up to 19,200 bps, up from 9,600 bps on the VT200s.

Like the VT200's, the VT300's included a number of alternate character sets for various international uses and basic form graphics. The system shipped with five sets of 94 characters, as well as a single set with 96 graphics characters. The sets were ASCII, ISO Latin and three graphics character sets. Using sixels, any one of these sets could be replaced with user-generated characters. The system also included DEC's unique National Replacement Character Sets that allowed single characters in a set to be swapped out to match the layout of a keyboard. For instance, in the UK the # symbol could be swapped out for the £, eliminating the need for custom versions of the terminal for each country. It supported the full range of ANSI escape codes, although some sources state it did not decode standard color sequences even on the VT340.

The screen itself was a 14 inch CRT mounted on a tilt and swivel stand. It offered a resolution of 800 by 500, and a number of different glyphs could be used to produce 25 lines or either 80 or 132 columns of text, the 25th line normally being used to display status codes, like caps lock, generated locally in the terminal. The VT330 had two bit-planes that produced 4 shades of grey, while the VT340 had four bit-planes to produce 16 colors out of a palette of 4096.

As was the case with all DEC terminals, the VT300 series could emulate previous models. The 300's could be set to VT100 or VT52 mode.
===
The VT420 was an ANSI standard computer terminal introduced in 1990 by Digital Equipment Corporation (DEC). The 420 was the only model in the 400 series, replacing the VT320. There were no color or graphics-capable 400 series terminals; the VT340 remained in production for those requiring ReGIS and Sixel graphics and color support. The entire lineup of VT300s and VT420 was eventually replaced by the relatively unknown VT500 series starting in 1993.

The VT420 consisted largely of a combination of the VT320 with the virtual terminal support of the VT330 and VT340. Those two models had included a system known as TD/SMP[a] which allowed two sessions to be multiplexed over a single serial connection to a compatible terminal server. Alternately the two sessions could be supported by separate serial connections on those models with multiple MMJ ports. Using either method, the VT330/340/420 could either show the two sessions behind each other, using a key sequence to flip back and forth, or split the screen horizontally to display the sessions one above the other, or vertically side-by-side.

The VT420 also added a number of more minor features. One was to add a number of PC character sets, allowing the terminal to be used with a variety of PC programs. Another allowed the terminal to generate the proper character sequences to produce rectangular-area commands. For instance, one could select a rectangular area and fill it with a particular character, or blank it out. This was in addition to the terminal-side editing system introduced on the VT300s.

The VT420 had a total of 5 sets of 94 characters for normal VT operation, another 3 sets of 128 PC characters, and 1 set of 96 characters containing various graphics and math symbols. Like all models since the VT200 series, the user could also upload a custom character set of their own design using the Sixel system. Likewise, it also supported the National Replacement Character Set system, which swapped out single characters in 7-bit modes to allow basic changes like swapping the # for the £ for use on UK systems.

All DEC terminals that came after the VT100, including the VT420, are able to emulate their ancestors. The VT420 had VT100 and VT52 emulating modes.

The screen itself was a 14" flat CRT with a resolution of 800 (horizontal) by 400 (vertical) pixels. A variety of glyphs were available that provided 80 or 132 characters across, and 24, 36 or 48 lines of text vertically. The screen had room for 25 lines at normal font sizes, but the last line was normally used for status indications, like ⇪ Caps Lock. The MMJ ports could operate at speeds up to 38,400 bit/s, double that of the VT300s maximum 19,200 bit/s.
===
The VT520 is an ANSI standard computer terminal introduced by Digital Equipment Corporation (DEC) in 1993 and 1994.[1] The VT520 supports monochrome graphics using the ReGIS vector or Sixel bitmap systems. The VT525 added color support, while the VT510 was a single-session, text-only version with a built-in monitor. The VT525 appears to be the most popular model in the series.

The VT500s replaced all previous models of DEC's VT line, at that time consisting of the VT420 text and VT340 graphics terminals. It was introduced in an era when the market was being flooded by low-cost IBM PC clones which could perform the same functions using a terminal emulator while also running other software. DEC introduced the VT500s only a short time before selling off their entire terminal division in August 1995. This brought the VT series to a close, after a total of about six million terminals had been sold.

The VT520 is still available from Boundless Technologies.

Description[edit]
In terms of major features, DEC's terminal line reached its peak with the VT300 series of 1988. The high-end models, the VT330 and VT340, included the abilities to display bitmap graphics using the sixel format, vector graphics using ReGIS or Tektronix 4010 emulation, terminal-side buffering and editing, and added the new ability to support two separate terminal sessions using a system known as TD/SMP.[a] The base-model VT320 was a simple text-only version that lacked TD/SMP support, and this was replaced by the VT420 in 1990, adding this feature.

By the mid-1990s the price of low-end PCs was rapidly falling to the under-$1000 price point. When equipped with a terminal emulator, these machines could perform all the functions of a DEC terminal, as well as running software locally. The terminal market began to crash, but remained important to DEC's core minicomputer business. DEC responded by introducing the VT500 series as simplified and lower-cost options. The VT510 was introduced in 1993 as an all-in-one unit like their previous designs, replacing DECs proprietary keyboards with a PS/2 port and adding standard RS-232 ports in addition to their proprietary MMJ serial ports. The 520 and 525 dispensed with the monitor as well, packaging the system into a pizza box case and working with a user-supplied monitor connected on an SVGA port.

Like all models of the VT series, the VT500's primary purpose is to act as an ANSI standard terminal. The VT510 supported only a single session, while the 520 and 525 supported up to four sessions, up from two in the earlier VT series. The user can flip between the sessions using control sequences on the keyboard (typically F4), or display multiple sessions at the same time by splitting the screen horizontally or vertically. All models have multiple character sets in ROM, supporting DEC, international and PC characters. They can also replace any of these by downloading custom characters using sixels, and perform single-character swaps using the National Replacement Character Set, swapping $ with £ for use with UK keyboards for instance.

The 500s included two MMJ connectors for serial connections, as well as a RS-232C port and a Centronics port for printing. The speed of the serial ports was increased to 115.2 kbps, up from 38.4 kbps on the VT300s. Any one of the serial ports could support two sessions using TD/SMP. Like earlier models of the VT line, the 500s could be put into modes emulating the VT100 and VT52, but added a wide variety of other emulations for Wyse, ADDS, Televideo and other terminals. The 500s also directly supported ANSI commands for color, like the Wyse, in addition to the custom escape sequences used for color support on previous VT models.

Another new feature was the inclusion of a set of desk accessories running on the terminal's CPU. These included a calculator, alarm clock, calendar and a character set viewer.

Terminal emulator specifications may refer to VT500 instead of VT510, VT520 and VT525 in the statements about their compatibility.

===
The VT50 was a CRT-based computer terminal introduced by Digital Equipment Corporation (DEC) in July 1974. It provided a display with 12 rows and 80 columns of upper-case text, and used an expanded set of control characters and forward-only scrolling based on the earlier VT05. DEC documentation of the era refers to the terminals as the DECscope, a name that was otherwise almost never seen.

The VT50 was sold only for a short period before it was replaced by the VT52 in September 1975.[1] The VT52 provided a screen of 24 rows and 80 columns of text and supported all 95 ASCII characters as well as 32 graphics characters, bi-directional scrolling, and an expanded control character system. DEC produced a series of upgraded VT52's with additional hardware for various uses.

The VT52 family was followed by the much more sophisticated VT100 in 1978.

The VT50 supported asynchronous communication at baud rates up to 9600 bits per second and did not require any fill characters. Like other early DEC terminals, the VT50 series were equipped with both an RS-232 port as well as a 20mA current loop, an older serial standard used with teletype machines that was more suitable to long-distance transmission over phone lines. Data was read into a small buffer, which the display hardware periodically read to produce the display.

To interpret the commands being sent in the serial data, it used a primitive central processing unit (CPU) built from small-scale-integration integrated circuits. It examined the data while the display hardware was inactive between raster scan lines, and then triggered the display hardware to take over at the appropriate time. The display system returned control to the CPU when it was complete. The CPU was so basic that addition and subtraction could only be done by repeatedly incrementing or decrementing two registers. Moreover, the time taken by such a loop had to be nearly constant, or text lower on the screen would be displayed in the wrong place during that refresh.

Characters typed on the keyboard were likewise stored in a buffer and read back over the serial line as quickly as possible. One notable feature of the VT50 was the introduction of a separate function keypad with the "Gold Key", which was used for editing programs like WPS-8, KED, and EDT. Pressing the Gold Key and then typing one of the keys on the keyboard sent a command sequence back to the host computer.

DEC also offered an optional hard-copy device called an electrolytic copier, which fit into the blank panel on the right side of the display. This device was able to print, scan-line by scan-line, an exact replica of the screen onto a damp roll of special paper. It did this by electroplating metal from an electrode into the paper.[2][3] The paper ran between two electrodes. The electrode on one side was a thin straight bar oriented across the paper width. The electrode on the other side was a thin helical bar wrapped around a rotating drum. One rotation of the drum scanned an intersecting area of the electrodes across the width of the paper. While the copier did an admirable job of capturing the contents of the screen, the output of the copier had an unfortunate resemblance to wet toilet tissue.[4][5] Digital patented the innovation of having a single character generator provide the text font for both screen and copier.

The basic layout of the terminal, with the screen and main keyboard on the left and the blank area on the right, was intended to allow the system to be upgraded. The printer was one such upgrade, but over time DEC offered a number of other options. The large size of the cabinet was deliberate, to avoid a cooling fan. The two circuit boards with processor and memory at the base of the terminal, and a single board with power-supply and monitor electronics at the rear, were cooled by convection. The large, flat top of the terminal frequently accommodated large volumes of DEC documentation, which could block the vents and cause overheating.

===
WPS-8 was the name of a Word Processing System sold by Digital Equipment Corporation for use with their PDP-8 processors (including the VT78, VT278 DECmate, and PC238 DECmate II and PC24P DECmate III microcomputer systems).

There is a webpage purporting to provide a downloadable word processing system manual located at http://manx.classiccmp.org/details/1,4005 (visited July 23, 2014).

WPS-8 supported a variety of 24 row by 80 or 132 column terminals including the VT52 family as well as the VT100 family and all subsequent ANSI-compatible terminals. A series of hierarchical menus allowed the user to command the system; the particular style of these menus became very-widely used by Digital, particularly within their "ALL-IN-1" office system. Once a document was opened for editing, near WYSIWYG editing was provided using a ruler to indicate the text alignment and tab stops for any given portion of the text. A typical editing session might have looked like this:

  Text conformed to the
  preceding ruler (now offscreen).
----L----T-----------------------------T---------------------------------------------R----
    Text conformed to the ruler shown just above
    L -> Left margin
    R -> Ragged-right margin
    T -> Left-aligned tab
---------L----------------------------->---------------------.-----------------------J----
         > -> Right-aligned tab
         . -> Decimal-aligned tab
         J -> Justified right margin (not justified on screen, only on printout)

              Aligned with the right tab
              Decimal-point aligned                        $5.99
                                                         1279.99

Using these various rulers, complex formatting could be achieved, even using a simple input device like a 24x80 character terminal. On ANSI terminals, character attributes such as bold and underline were shown on the screen. On the VT52 terminals (which could not display attributes), the operator could perform the same functions but only the printout would reveal the formatting.

As text was typed-in, the system automatically word-wrapped the text so that it conformed to the ruler currently in effect for that section of the document. Rulers could be added or modified and the text from that ruler forward to the next would automatically be adjusted to conform to the new ruler. Hyphenation could be semi-automatically performed (including "hidden" hyphens that would only be revealed if a line break exposed them).

Specialized editing functions were provided using the terminal keypad. A few functions could be commanded simply by pressing a keypad key, but a far wider range of functions could be commanded by prefixing them with the "Gold Key" (the PF1 key on the keypad, colored gold on systems equipped with the WPS-8 custom keycaps). This style of "gold key" editing also became endemic at Digital, later showing up in mainstream general-purpose text editors such as KED and EDT as well as the before-mentioned "ALL-IN-1" office system. The editing facilities included making a selection and then using cut and paste (much like today's word processors, but using keys marked for cut and paste, rather than a mouse).

Printing was to any of several different letter-quality daisy wheel printers including a DEC variant of the Diablo 630.

WPS-8 normally ran from a single floppy diskette and user data could be stored on the system diskette or additional data-only diskettes. Up to four diskette drives were supported in a single system.

The system also supported the creation of data tables, the sorting of these data tables, arithmetic calculations using these data, and a mail-merge operation using these data and the arithmetic results. Through the extensive use of overlays, it managed all that on a 12-bit, 1.2-μs processor with 16 KWords of memory and 256 KB of diskette storage. The limited resources of the system did not permit a spell-checker, though, primarily because there was no place with adequate storage to contain the dictionary file.
===
The Gold key is a computer keyboard key used as a prefix to invoke a variety of single-key editing and formatting functions. Usually located in the top-left position of the numeric keypad on platforms such as the VT100, it is the signature element of a consistent user interface implemented by Digital Equipment Corporation across multiple product lines.

It is used within WPS, EDT, and many other common VAX programs.[1] The key, typically located as the upper leftmost key on the numeric keypad on different terminals,[2] was not necessarily colored gold. Some Digital Equipment Corporation (DEC) terminals would include keyboards where the gold key was labeled PF1, as on the VT100 and VT200, or was colored blue, as on the VT52.[3] On some keyboards, the normal function of a key would be labeled on the lower portion of the key, while its alternate function activated with the GOLD key would be labeled above it.[2]

The Gold Key is used to invoke single-key functions which may be located on either the main keyboard or the numeric keypad. For example, on the WPS-8 word processing system, the main keyboard key C is marked “CENTR”, in gold lettering, on its front surface; the keystrokes GOLDC invoke that word processing function to center the current line of text.[4]:p1.8

The Gold key is a prefix key, not a modifier key. A modifier key would be pressed and held while a second key is pressed; the Gold key is pressed and released before a second key is pressed and released. In that sense, DEC and compatible software uses the Gold key in the same way that Emacs uses the escape key.
===
The Tektronix 401x series was a family of text and graphics computer terminals based on the company's storage tube technology. The 4000 series were less expensive (under $10,000[1]) than earlier graphics terminals, such as the IBM 2250 because no additional electronics were needed to maintain the display on the screen (beyond providing proper voltages to it). They were widely used in the CAD market in the 1970s and early 1980s. There were several members of the family introduced through the 1970s, the best known being the 4010 and 4014. They remained popular until the introduction of inexpensive graphics workstations in the 1980s. The new graphics workstations used raster displays and dedicated screen buffers that became more affordable as solid state memory chips became cheaper.

===
Tektronix, Inc. is an American company best known for manufacturing test and measurement devices such as oscilloscopes, logic analyzers, and video and mobile test protocol equipment. In November 2007, Danaher Corporation acquired Tektronix as a subsidiary. The company received a 2007 Technology & Engineering Emmy Awards for compliance standards monitoring systems for Advanced Television Systems Committee (ATSC) standard and Digital Video Broadcasting (DVB) transport streams. Several charities are or were associated with Tektronix, including the Tektronix Foundation and the M.J. Murdock Charitable Trust in Vancouver, Washington.[citation needed]
===
Web Distributed Authoring and Versioning (WebDAV) is an extension of the Hypertext Transfer Protocol (HTTP) that allows clients to perform remote Web content authoring operations. A working group of the Internet Engineering Task Force (IETF) defined WebDAV in RFC 4918.

The WebDAV protocol provides a framework for users to create, change and move documents on a server, typically a web server or web share. The most important features of the WebDAV protocol include the maintenance of properties about an author or modification date, namespace management, collections, and overwrite protection. Maintenance of properties includes such things as the creation, removal, and querying of file information. Namespace management deals with the ability to copy and move web pages within a server’s namespace. Collections deal with the creation, removal, and listing of various resources. Lastly, overwrite protection handles aspects related to locking of files.

The WebDAV working group concluded its work in March 2007, after the Internet Engineering Steering Group (IESG) accepted an incremental update to RFC 2518. Other extensions left unfinished at that time, such as the BIND method, have been finished by their individual authors, independent of the formal working group.
===
sabre/dav is an open source WebDAV server, developed by fruux built in PHP. It is an implementation of the WebDAV protocol (with extensions for CalDAV[1] and CardDAV), providing a native PHP server implementation which operates on Apache 2 and Nginx web servers.
===
Calendaring Extensions to WebDAV, or CalDAV, is an Internet standard allowing a client to access scheduling information on a remote server. It extends WebDAV (HTTP-based protocol for data manipulation) specification and uses iCalendar format for the data. The access protocol is defined by RFC 4791. It allows multiple client access to the same information thus allowing cooperative planning and information sharing. Many server and client applications support the protocol. Extensions to CalDAV for automated scheduling are also standardized, as RFC 6638.
===
iCalendar is a computer file format which allows Internet users to send meeting requests and tasks to other Internet users, via email, or sharing files with an extension of .ics. Recipients of the iCalendar data file (with supporting software, such as an email client or calendar application) can respond to the sender easily or counter-propose another meeting date/time.[1]

iCalendar is used and supported by a large number of products, including Google Calendar, Apple Calendar (formerly iCal),[2] IBM Lotus Notes,[3] Yahoo! Calendar, Evolution (software), eM Client, Lightning extension for Mozilla Thunderbird and SeaMonkey, and partially by Microsoft Outlook and Novell GroupWise.

iCalendar is designed to be independent of the transport protocol. For example, certain events can be sent by traditional email or whole calendar files can be shared and edited by using a WebDav server, or SyncML. Simple web servers (using just the HTTP protocol) are often used to distribute iCalendar data about an event and to publish busy times of an individual. Publishers can embed iCalendar data in web pages using hCalendar, a 1:1 microformat representation of iCalendar in semantic (X)HTML.
===
hCalendar (short for HTML iCalendar) is a microformat standard for displaying a semantic (X)HTML representation of iCalendar-format calendar information about an event, on web pages, using HTML classes and rel attributes.

It allows parsing tools (for example other websites, or browser add-ons[1] like Firefox's Operator extension) to extract the details of the event, and display them using some other website, index or search them, or to load them into a calendar or diary program, for instance. Multiple instances can be displayed as timelines.
===
SyncML (Synchronization Markup Language) is the former name for a platform-independent information synchronization standard. The project is currently referred to as Open Mobile Alliance Data Synchronization and Device Management. The purpose of SyncML is to offer an open standard as a replacement for existing data synchronization solutions, which have mostly been somewhat vendor-, application- or operating system specific.
===
GroupWise is a messaging and collaboration platform from Novell that supports email, calendaring, personal information management, instant messaging, and document management. The GroupWise platform consists of desktop client software, which is available for Windows, Mac OS X, and Linux, and the server software, which is supported on Windows Server and Linux.

The platform also supports WebAccess, its browser-based webmail client. Mobile access to messaging, calendaring, contacts and other data from smartphones and tablet computers is supported (through the Novell Data Synchronizer server software) via the Exchange ActiveSync protocol. Enterprise instant messaging and presence is handled by Novell Messenger, which integrates with GroupWise.

The latest generation of the platform is GroupWise 2014.

Name					Version Number	Release Date
WordPerfect Library		-				-	
WordPerfect Office		2.0				1988	
WordPerfect Office		3.0				1990	
WordPerfect Office		3.1				1992	
WordPerfect Office		4.0				1993	
GroupWise				4.1				1994	
GroupWise				5				1996	
GroupWise				5.1				1997	
GroupWise				5.2				1997	
GroupWise				5.5				1998	
GroupWise				6.0				2001	
GroupWise				6.5				2003	
GroupWise				7.0				2005	
GroupWise				8.0				2008	
GroupWise				2012			2012	
GroupWise				2014			2014

===
Base64 is a group of similar binary-to-text encoding schemes that represent binary data in an ASCII string format by translating it into a radix-64 representation. The term Base64 originates from a specific MIME content transfer encoding.

Base64 encoding schemes are commonly used when there is a need to encode binary data that needs to be stored and transferred over media that is designed to deal with textual data. This is to ensure that the data remains intact without modification during transport. Base64 is commonly used in a number of applications, including email via MIME, and storing complex data in XML.
===
WebGL (Web Graphics Library) is a JavaScript API for rendering interactive 3D computer graphics and 2D graphics within any compatible web browser without the use of plug-ins.[2] WebGL is integrated completely into all the web standards of the browser allowing GPU accelerated usage of physics and image processing and effects as part of the web page canvas. WebGL elements can be mixed with other HTML elements and composited with other parts of the page or page background.[3] WebGL programs consist of control code written in JavaScript and shader code that is executed on a computer's Graphics Processing Unit (GPU). WebGL is designed and maintained by the non-profit Khronos Group.[4]
===

Google Panda is a change to Google's search results ranking algorithm that was first released in February 2011. The change aimed to lower the rank of "low-quality sites" or "thin sites",[1] and return higher-quality sites near the top of the search results. CNET reported a surge in the rankings of news websites and social networking sites, and a drop in rankings for sites containing large amounts of advertising.[2] This change reportedly affected the rankings of almost 12 percent of all search results.[3] Soon after the Panda rollout, many websites, including Google's webmaster forum, became filled with complaints of scrapers/copyright infringers getting better rankings than sites with original content. At one point, Google publicly asked for data points to help detect scrapers better.[4] Google's Panda has received several updates since the original rollout in February 2011, and the effect went global in April 2011. To help affected publishers, Google provided an advisory on its blog,[5] thus giving some direction for self-evaluation of a website's quality. Google has provided a list of 23 bullet points on its blog answering the question of "What counts as a high-quality site?" that is supposed to help webmasters "step into Google's mindset".[6]

Google Panda is a filter that prevents low quality sites and/or pages from ranking well in the search engine results page. The filter's threshold is influenced by Google Quality Raters.[7] Quality Raters answer questions such as "would I trust this site with my credit card?" so that Google can distinguish the difference between high and low quality sites.[8]

The Google Panda patent (patent 8,682,892), filed on September 28, 2012, was granted on March 25, 2014. The patent states that Google Panda creates a ratio with a site's inbound links and reference queries, search queries for the site's brand. That ratio is then used to create a sitewide modification factor. The sitewide modification factor is then used to create a modification factor for a page based upon a search query. If the page fails to meet a certain threshold, the modification factor is applied and, therefore, the page would rank lower in the search engine results page.[9]

Google Panda affects the ranking of an entire site or a specific section rather than just the individual pages on a site.[10]

In March 2012, Google updated Panda.

Google says it only takes a few pages of poor quality or duplicated content to hold down traffic on an otherwise solid site, and recommends such pages be removed, blocked from being indexed by the search engine, or rewritten.[11] However, Matt Cutts, head of webspam at Google, warns that rewriting duplicate content so that it is original may not be enough to recover from Panda, the rewrites must be of sufficiently high quality, as such content brings "additional value" to the web. Content that is general, non-specific, and not substantially different from what is already out there should not be expected to rank well: "Those other sites are not bringing additional value. While they’re not duplicates they bring nothing new to the table."

===
A Google penalty is the negative impact on a website's search rankings based on updates to Google's search algorithms and/or manual review. The penalty can be an unfortunate by-product of an algorithm update or an intentional penalization for various black-hat SEO techniques.

Google penalizes sites for engaging in practices that are against its webmaster guidelines. These penalties can be the result of a manual review or algorithm updates such as Google Penguin.[1]

Google penalties can result in the drop of rankings for every page of a site, for a specific keyword, or for a specific page. Any drop in rankings brings with it a major drop in traffic for the site.[citation needed]

To find out if a website has been affected by a Google penalty, website owners can use Google Webmaster Tools as well as analyze the timing of their traffic drop with the timing of known Google updates.[2]


Google has been updating its algorithm for as long as it has been fighting the manipulation of organic search results. However, up until May 10, 2012, when Google launched the Google Penguin update, many people wrongly believed that low-quality backlinks would not negatively affect ranks. While this viewpoint was common, it was not correct, as Google had been applying such link-based penalties[3] for many years, but not made public how the company approached and dealt with what they called "link spam". Since this time there has been a much wider acknowledgement about the dangers of bad SEO and a forensic analysis of backlinks to ensure there are no harmful links.
===
Google Penguin is a codename[1] for a Google algorithm update that was first announced on April 24, 2012. The update is aimed at decreasing search engine rankings of websites that violate Google’s Webmaster Guidelines[2] by using now declared black-hat SEO techniques involved in increasing artificially the ranking of a webpage by manipulating the number of links pointing to the page. Such tactics are commonly described as link schemes.[3] According to Google's John Mueller, Google has announced all updates to the Penguin filter to the public.[4]

By Google’s estimates,[5] Penguin affects approximately 3.1% of search queries in English, about 3% of queries in languages like German, Chinese, and Arabic, and an even bigger percentage of them in "highly spammed" languages. On May 25, 2012, Google unveiled another Penguin update, called Penguin 1.1. This update, according to Matt Cutts, was supposed to affect less than one-tenth of a percent of English searches. The guiding principle for the update was to penalize websites using manipulative techniques to achieve high rankings. The purpose per Google was to catch excessive spammers. Allegedly, few websites lost search rankings on Google for specific keywords during the Panda and Penguin rollouts.[according to whom?] Google specifically mentions that doorway pages, which are only built to attract search engine traffic, are against their webmaster guidelines.

In January 2012, the so-called Page Layout Algorithm Update[6] (also known as the Top Heavy Update)[7] was released, which targeted websites with too many ads, or too little content above the fold.

Penguin 3 was released October 5, 2012 and affected 0.3% of queries.[8] Penguin 4 (AKA Penguin 2.0) was released on May 22, 2013 and affected 2.3% of queries.[9] Penguin 5 (AKA Penguin 2.1)[10] was released on October 4, 2013, affected around 1% of queries, and has been the most recent of the Google Penguin algorithm updates.

Google may have released Penguin 3.0 on October 18, 2014.[11]

On October 21, 2014, Google's Pierre Farr confirmed that Penguin 3.0 was an algorithm "refresh", with no new signals added.[12]

On April 7th 2015, Google's John Mueller said in a Google+ hangout that both Penguin and Panda "currently are not updating the data regularly" and that they need to kind of be pushed out manually. This confirms that the algorithm is not updated continuously which was believed to be the case earlier on in the year.[13]

The strategic goal that Panda, Penguin, and the page layout update share is to display higher quality websites at the top of Google’s search results. However, sites that were downranked as the result of these updates have different sets of characteristics. The main target of Google Penguin is spamdexing (including link bombing).

===
In computing, spamdexing (also known as search engine spam, search engine poisoning, Black-Hat SEO, search spam or web spam)[1] is the deliberate manipulation of search engine indexes. It involves a number of methods, such as repeating unrelated phrases, to manipulate the relevance or prominence of resources indexed in a manner inconsistent with the purpose of the indexing system.[2][3] It could be considered to be a part of search engine optimization, though there are many search engine optimization methods that improve the quality and appearance of the content of web sites and serve content useful to many users.[4] Search engines use a variety of algorithms to determine relevancy ranking. Some of these include determining whether the search term appears in the body text or URL of a web page. Many search engines check for instances of spamdexing and will remove suspect pages from their indexes. Also, people working for a search-engine organization can quickly block the results-listing from entire websites that use spamdexing, perhaps alerted by user complaints of false matches. The rise of spamdexing in the mid-1990s made the leading search engines of the time less useful. Using unethical methods to make websites rank higher in search engine results than they otherwise would is commonly referred to in the SEO (Search Engine Optimization) industry as "Black Hat SEO."[5]

Common spamdexing techniques can be classified into two broad classes: content spam[4] (or term spam) and link spam.[3]
===
Google Hummingbird[1][2] is a search algorithm used by Google.

Google started using Hummingbird about August 30, 2013,[3] and announced the change on September 26[4] on the eve of the company's 15th anniversary.[5]

Gianluca Fiorelli said Hummingbird is about synonyms but also about context. Google always had synonyms, he writes, but with Hummingbird it is also able to judge context - thereby judging the intent of a person carrying out a search, to determine what they are trying to find out.[6] This concept is called semantic search.

Danny Sullivan said of Hummingbird, "Google said that Hummingbird is paying more attention to each word in a query, ensuring that the whole query — the whole sentence or conversation or meaning — is taken into account."[7] Michelle Hill said Hummingbird is about "understanding intent".[8] Steve Masters wrote, "The Hummingbird approach should be inspirational to anyone managing and planning content — if you aren't already thinking like Hummingbird, you should be. In a nutshell, think about why people are looking for something rather than what they are looking for. A content strategy should be designed to answer their needs, not just provide them with facts."[9]

The Hummingbird update was the first major update to Google's search algorithm since the 2010 “Caffeine Update”, but even that was limited primarily to improving the indexing of information rather than the sorting of information. Google search chief Amit Singhal stated that Hummingbird is the first major update of its type since 2001.[10][11]

Conversational search leverages natural language, semantic search, and more to improve the way search queries are parsed.[12] Unlike previous search algorithms which would focus on each individual word in the search query, Hummingbird considers each word but also how each word makes up the entirety of the query — the whole sentence or conversation or meaning — is taken into account, rather than particular words. The goal is that pages matching the meaning do better, rather than pages matching just a few words.[13][2]

Much like an extension of Google's "Knowledge Graph", Hummingbird is aimed at making interactions more human — capable of understanding the concepts and relationships between keywords. [14]

Hummingbird places greater emphasis on page content making search results more relevant and pertinent and ensuring that Google delivers users to the most appropriate page of a website, rather than to a home page or top level page.[15]
===


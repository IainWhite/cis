
ReGIS - Remote Graphic Instruction Set
ReGIS, short for Remote Graphic Instruction Set, was a vector graphics markup language developed by Digital Equipment Corporation ([DEC]) for later models of their famous VT series of computer terminals. ReGIS supported rudimentary vector graphics consisting of lines, circular arcs, and similar shapes. Terminals supporting ReGIS generally allowed graphics and text to be mixed on-screen, which made construction of graphs and charts relatively easy.

ReGIS was first introduced on the [VT125] in July 1981, followed shortly thereafter by the VK100 "GIGI" which combined the VT125 display system with composite video output and a [BASIC] interpreter. Later versions of the VT series included ReGIS, often with color support as well. This included the [VT240] and 241, VT330 and 340, and the VT525. ReGIS is also supported by a small number of terminal emulator systems.

ReGIS replaced an earlier system known as waveform graphics that had been introduced on the VT55 and later used on the VT105. DEC normally provided backward compatibility with their terminals, but in this case the waveform system was simply dropped when ReGIS was introduced.
===

DEC ALL-IN-1
ALL-IN-1 was an office automation product developed and sold by [Digital Equipment Corporation] in the 1980s. [HP] now maintains the current version Office Server V3.2 for [OpenVMS Alpha] and [OpenVMS VAX] systems.

ALL-IN-1 was advertised as an office automation system including functionality in Electronic Messaging, Word Processing and Time Management. It offered an application development platform and customisation capabilities that ranged from scripting to code-level integration.

ALL-IN-1 was designed and developed by Skip Walter, John Churin and Marty Skinner from Digital Equipment Corporation who began work in 1977. The first version of the software was called CP/OSS, the Charlotte Package of Office System Services, named after the location of the developers. In 1983, the product was renamed ALL-IN-1 and the Charlotte group continued to develop versions 1.1 through 1.3.

Digital then made the decision to move most of the development activity to its central engineering facility in Reading, United Kingdom, where a group there took responsibility for the product from version 2.0 (released in field test in 1984 and to customers in 1985) onward. The Charlotte group continued to work on the Time Management subsystem until version 2.3 and other contributions were made from groups based in Sophia-Antipolis, France (System for Customisation Management and the integration with VAX Notes), Reading (Message Router and MAILbus), and Spitbrook, New Hampshire (FMS). ALL-IN-1 V3.0 introduced shared file cabinets and the File Cabinet Server (FCS) to lay the foundation for an eventual integration with TeamLinks, Digital's PC office client. Previous integrations with PCs included PC ALL-IN-1, a [DOS] based product introduced in 1989 that never proved popular with customers.
===

The Gold Key
The Gold key is a computer keyboard key used as a prefix to invoke a variety of single-key editing and formatting functions. Usually located in the top-left position of the numeric keypad on platforms such as the [VT100], it is the signature element of a consistent user interface implemented by [Digital Equipment Corporation] across multiple product lines.

It is used within WPS, EDT, and many other common [VAX] programs. The key, typically located as the upper leftmost key on the numeric keypad on different terminals, was not necessarily coloured gold. Some Digital Equipment Corporation (DEC) terminals would include keyboards where the gold key was labeled PF1, as on the VT100 and [VT200], or was coloured blue, as on the [VT52]. On some keyboards, the normal function of a key would be labeled on the lower portion of the key, while its alternate function activated with the GOLD key would be labeled above it.

The Gold Key is used to invoke single-key functions which may be located on either the main keyboard or the numeric keypad. For example, on the [WPS-8] word processing system, the main keyboard key C is marked “CENTR”, in gold lettering, on its front surface; the keystrokes GOLDC invoke that word processing function to center the current line of text.

The Gold key is a prefix key, not a modifier key. A modifier key would be pressed and held while a second key is pressed; the Gold key is pressed and released before a second key is pressed and released. In that sense, DEC and compatible software uses the Gold key in the same way that [Emacs] uses the escape key.
===

Tektronix 401x
The Tektronix 401x series was a family of text and graphics computer terminals based on the company's storage tube technology. The 4000 series were less expensive (under $10,000) than earlier graphics terminals, such as the IBM 2250 because no additional electronics were needed to maintain the display on the screen (beyond providing proper voltages to it). They were widely used in the [CAD] market in the 1970s and early 1980s. There were several members of the family introduced through the 1970s, the best known being the 4010 and 4014. They remained popular until the introduction of inexpensive graphics workstations in the 1980s. The new graphics workstations used raster displays and dedicated screen buffers that became more affordable as solid state memory chips became cheaper.
===

Tektronix
Tektronix Inc is an American company best known for manufacturing test and measurement devices such as oscilloscopes, logic analysers, and video and mobile test protocol equipment. In November 2007, Danaher Corporation acquired Tektronix as a subsidiary. The company received a 2007 Technology & Engineering Emmy Awards for compliance standards monitoring systems for Advanced Television Systems Committee (ATSC) standard and Digital Video Broadcasting (DVB) transport streams. Several charities are or were associated with Tektronix, including the Tektronix Foundation and the M.J. Murdock Charitable Trust in Vancouver, Washington.
===

WebDAV - Web Distributed Authoring and Versioning
Web Distributed Authoring and Versioning (WebDAV) is an extension of the Hypertext Transfer Protocol ([HTTP]) that allows clients to perform remote Web content authoring operations. A working group of the Internet Engineering Task Force ([IETF]) defined WebDAV in [RFC 4918].

The WebDAV protocol provides a framework for users to create, change and move documents on a server, typically a web server or web share. The most important features of the WebDAV protocol include the maintenance of properties about an author or modification date, namespace management, collections, and overwrite protection. Maintenance of properties includes such things as the creation, removal, and querying of file information. Namespace management deals with the ability to copy and move web pages within a server’s namespace. Collections deal with the creation, removal, and listing of various resources. Lastly, overwrite protection handles aspects related to locking of files.

The WebDAV working group concluded its work in March 2007, after the Internet Engineering Steering Group ([IESG]) accepted an incremental update to [RFC 2518]. Other extensions left unfinished at that time, such as the BIND method, have been finished by their individual authors, independent of the formal working group.
===

SabreDAV 
SabreDAV is an open source [WebDAV] server, developed by fruux built in [PHP]. It is an implementation of the WebDAV protocol (with extensions for [CalDAV] and CardDAV), providing a native PHP server implementation which operates on [Apache 2] and [Nginx] web servers.
===

CalDAV - Calendaring Extensions to WebDAV
Calendaring Extensions to WebDAV, or CalDAV, is an Internet standard allowing a client to access scheduling information on a remote server. It extends [WebDAV] ([HTTP] based protocol for data manipulation) specification and uses [iCalendar] format for the data. The access protocol is defined by [RFC 4791]. It allows multiple client access to the same information thus allowing cooperative planning and information sharing. Many server and client applications support the protocol. Extensions to CalDAV for automated scheduling are also standardised, as [RFC 6638].
===

iCalendar
iCalendar is a computer file format which allows Internet users to send meeting requests and tasks to other Internet users, via email, or sharing files with an extension of .ics. Recipients of the iCalendar data file (with supporting software, such as an email client or calendar application) can respond to the sender easily or counter propose another meeting date / time.

iCalendar is used and supported by a large number of products, including [Google Calendar], Apple Calendar (formerly [iCal]), [IBM Lotus Notes], [Yahoo! Calendar], Evolution (software), eM Client, Lightning extension for [Mozilla Thunderbird] and [SeaMonkey], and partially by [Microsoft Outlook] and [Novell GroupWise].

iCalendar is designed to be independent of the transport protocol. For example, certain events can be sent by traditional email or whole calendar files can be shared and edited by using a WebDav server, or [SyncML]. Simple web servers (using just the HTTP protocol) are often used to distribute iCalendar data about an event and to publish busy times of an individual. Publishers can embed iCalendar data in web pages using [hCalendar], a 1:1 microformat representation of iCalendar in semantic (X)HTML.
===

hCalendar
hCalendar (short for HTML iCalendar) is a microformat standard for displaying a semantic (X)HTML representation of [iCalendar] format calendar information about an event, on web pages, using HTML classes and rel attributes.

It allows parsing tools (for example other websites, or browser add-ons like [Firefox]'s Operator extension) to extract the details of the event, and display them using some other website, index or search them, or to load them into a calendar or diary program, for instance. Multiple instances can be displayed as timelines.
===

SyncML (Synchronization Markup Language) 
SyncML (Synchronization Markup Language) is the former name for a platform independent information synchronisation standard. The project is currently referred to as Open Mobile Alliance Data Synchronisation and Device Management. The purpose of SyncML is to offer an open standard as a replacement for existing data synchronisation solutions, which have mostly been somewhat vendor, application or operating system specific.
===

Novell GroupWise
GroupWise is a messaging and collaboration platform from [Novell] that supports email, calendaring, personal information management, instant messaging, and document management. The GroupWise platform consists of desktop client software, which is available for [Windows], [Mac OS X], and [Linux], and the server software, which is supported on Windows Server and Linux.

The platform also supports WebAccess, its browser-based webmail client. Mobile access to messaging, calendaring, contacts and other data from smartphones and tablet computers is supported (through the Novell Data Synchroniser server software) via the Exchange ActiveSync protocol. Enterprise instant messaging and presence is handled by Novell Messenger, which integrates with GroupWise.

Versions
Name					Version Number	Release Date
WordPerfect Library		-				-	
WordPerfect Office		2.0				1988	
WordPerfect Office		3.0				1990	
WordPerfect Office		3.1				1992	
WordPerfect Office		4.0				1993	
GroupWise				4.1				1994	
GroupWise				5				1996	
GroupWise				5.1				1997	
GroupWise				5.2				1997	
GroupWise				5.5				1998	
GroupWise				6.0				2001	
GroupWise				6.5				2003	
GroupWise				7.0				2005	
GroupWise				8.0				2008	
GroupWise				2012			2012	
GroupWise				2014			2014

===

Base64
Base64 is a group of similar binary-to-text encoding schemes that represent binary data in an [ASCII] string format by translating it into a radix-64 representation. The term Base64 originates from a specific [MIME] content transfer encoding.

Base64 encoding schemes are commonly used when there is a need to encode binary data that needs to be stored and transferred over media that is designed to deal with textual data. This is to ensure that the data remains intact without modification during transport. Base64 is commonly used in a number of applications, including email via MIME, and storing complex data in [XML].
===

WebGL
WebGL (Web Graphics Library) is a [JavaScript] API for rendering interactive 3D computer graphics and 2D graphics within any compatible web browser without the use of plug-ins. WebGL is integrated completely into all the web standards of the browser allowing GPU accelerated usage of physics and image processing and effects as part of the web page canvas. WebGL elements can be mixed with other [HTML] elements and composited with other parts of the page or page background. WebGL programs consist of control code written in JavaScript and shader code that is executed on a computer's Graphics Processing Unit (GPU). WebGL is designed and maintained by the non-profit Khronos Group.
===

Google Panda
Google Panda is a change to [Google]'s search results ranking algorithm that was first released in February 2011. The change aimed to lower the rank of "low-quality sites" or "thin sites", and return higher quality sites near the top of the search results. CNET reported a surge in the rankings of news websites and social networking sites, and a drop in rankings for sites containing large amounts of advertising. This change reportedly affected the rankings of almost 12 percent of all search results. Soon after the Panda rollout, many websites, including Google's webmaster forum, became filled with complaints of scrapers / copyright infringers getting better rankings than sites with original content. At one point, Google publicly asked for data points to help detect scrapers better. Google's Panda has received several updates since the original rollout in February 2011, and the effect went global in April 2011. To help affected publishers, Google provided an advisory on its blog, thus giving some direction for self-evaluation of a website's quality. Google has provided a list of 23 bullet points on its blog answering the question of "What counts as a high-quality site?" that is supposed to help webmasters "step into Google's mindset".

Google Panda is a filter that prevents low quality sites and / or pages from ranking well in the search engine results page. The filter's threshold is influenced by Google Quality Raters. Quality Raters answer questions such as "would I trust this site with my credit card?" so that Google can distinguish the difference between high and low quality sites.

The Google Panda patent (patent 8,682,892), filed on 28 September 2012, was granted on 25 March 2014. The patent states that Google Panda creates a ratio with a site's inbound links and reference queries, search queries for the site's brand. That ratio is then used to create a sitewide modification factor. The sitewide modification factor is then used to create a modification factor for a page based upon a search query. If the page fails to meet a certain threshold, the modification factor is applied and, therefore, the page would rank lower in the search engine results page.

Google Panda affects the ranking of an entire site or a specific section rather than just the individual pages on a site.

In March 2012, Google updated Panda.

Google says it only takes a few pages of poor quality or duplicated content to hold down traffic on an otherwise solid site, and recommends such pages be removed, blocked from being indexed by the search engine, or rewritten. However, Matt Cutts, head of webspam at Google, warns that rewriting duplicate content so that it is original may not be enough to recover from Panda, the rewrites must be of sufficiently high quality, as such content brings "additional value" to the web. Content that is general, non-specific, and not substantially different from what is already out there should not be expected to rank well: "Those other sites are not bringing additional value. While they’re not duplicates they bring nothing new to the table."

===

Google Penalty 
A Google penalty is the negative impact on a website's search rankings based on updates to [Google]'s search algorithms and / or manual review. The penalty can be an unfortunate by-product of an algorithm update or an intentional penalisation for various black-hat SEO techniques.

Google penalises sites for engaging in practices that are against its webmaster guidelines. These penalties can be the result of a manual review or algorithm updates such as [Google Penguin].

Google penalties can result in the drop of rankings for every page of a site, for a specific keyword, or for a specific page. Any drop in rankings brings with it a major drop in traffic for the site.

To find out if a website has been affected by a Google penalty, website owners can use Google Webmaster Tools as well as analyse the timing of their traffic drop with the timing of known Google updates.

Google has been updating its algorithm for as long as it has been fighting the manipulation of organic search results. However, up until May 10, 2012, when Google launched the Google Penguin update, many people wrongly believed that low-quality backlinks would not negatively affect ranks. While this viewpoint was common, it was not correct, as Google had been applying such link-based penalties for many years, but not made public how the company approached and dealt with what they called "link spam". Since this time there has been a much wider acknowledgement about the dangers of bad [SEO] and a forensic analysis of backlinks to ensure there are no harmful links.
===

Google Penguin
Google Penguin is a codename for a [Google] algorithm update that was first announced on 24 April 2012. The update is aimed at decreasing search engine rankings of websites that violate Google’s Webmaster Guidelines by using now declared black-hat SEO techniques involved in increasing artificially the ranking of a webpage by manipulating the number of links pointing to the page. Such tactics are commonly described as link schemes. According to Google's John Mueller, Google has announced all updates to the Penguin filter to the public.

By Google’s estimates, Penguin affects approximately 3.1% of search queries in English, about 3% of queries in languages like German, Chinese, and Arabic, and an even bigger percentage of them in "highly spammed" languages. On 25 May 2012, Google unveiled another Penguin update, called Penguin 1.1. This update, according to Matt Cutts, was supposed to affect less than one-tenth of a percent of English searches. The guiding principle for the update was to penalise websites using manipulative techniques to achieve high rankings. The purpose per Google was to catch excessive spammers. Allegedly, few websites lost search rankings on Google for specific keywords during the Panda and Penguin rollouts. Google specifically mentions that doorway pages, which are only built to attract search engine traffic, are against their webmaster guidelines.

In January 2012, the so-called Page Layout Algorithm Update (also known as the Top Heavy Update) was released, which targeted websites with too many ads, or too little content above the fold.

Penguin 3 was released 5 October 2012 and affected 0.3% of queries. Penguin 4 (AKA Penguin 2.0) was released on 22 May 2013 and affected 2.3% of queries. Penguin 5 (AKA Penguin 2.1) was released on 4 October 2013, affected around 1% of queries, and has been the most recent of the Google Penguin algorithm updates.
===

Spamdexing
In computing, spamdexing (also known as search engine spam, search engine poisoning, Black-Hat SEO, search spam or web spam) is the deliberate manipulation of search engine indexes. It involves a number of methods, such as repeating unrelated phrases, to manipulate the relevance or prominence of resources indexed in a manner inconsistent with the purpose of the indexing system. It could be considered to be a part of [search engine optimisation], though there are many search engine optimisation methods that improve the quality and appearance of the content of web sites and serve content useful to many users. Search engines use a variety of algorithms to determine relevancy ranking. Some of these include determining whether the search term appears in the body text or URL of a web page. Many search engines check for instances of spamdexing and will remove suspect pages from their indexes. Also, people working for a search-engine organisation can quickly block the results listing from entire websites that use spamdexing, perhaps alerted by user complaints of false matches. The rise of spamdexing in the mid 1990s made the leading search engines of the time less useful. Using unethical methods to make websites rank higher in search engine results than they otherwise would is commonly referred to in the SEO (Search Engine Optimisation) industry as "Black Hat SEO."

Common spamdexing techniques can be classified into two broad classes: content spam (or term spam) and link spam.
===

Google Hummingbird
Google Hummingbird is a search algorithm used by [Google].

Google started using Hummingbird about 30 August 2013, and announced the change on 26 September on the eve of the company's 15th anniversary.

Gianluca Fiorelli said Hummingbird is about synonyms but also about context. Google always had synonyms, he writes, but with Hummingbird it is also able to judge context - thereby judging the intent of a person carrying out a search, to determine what they are trying to find out. This concept is called semantic search.

Danny Sullivan said of Hummingbird, "Google said that Hummingbird is paying more attention to each word in a query, ensuring that the whole query — the whole sentence or conversation or meaning — is taken into account." Michelle Hill said Hummingbird is about "understanding intent". Steve Masters wrote, "The Hummingbird approach should be inspirational to anyone managing and planning content — if you aren't already thinking like Hummingbird, you should be. In a nutshell, think about why people are looking for something rather than what they are looking for. A content strategy should be designed to answer their needs, not just provide them with facts".

The Hummingbird update was the first major update to Google's search algorithm since the 2010 “Caffeine Update”, but even that was limited primarily to improving the indexing of information rather than the sorting of information. Google search chief Amit Singhal stated that Hummingbird is the first major update of its type since 2001.

Conversational search leverages natural language, semantic search, and more to improve the way search queries are parsed. Unlike previous search algorithms which would focus on each individual word in the search query, Hummingbird considers each word but also how each word makes up the entirety of the query — the whole sentence or conversation or meaning — is taken into account, rather than particular words. The goal is that pages matching the meaning do better, rather than pages matching just a few words.

Much like an extension of Google's "Knowledge Graph", Hummingbird is aimed at making interactions more human — capable of understanding the concepts and relationships between keywords.

Hummingbird places greater emphasis on page content making search results more relevant and pertinent and ensuring that Google delivers users to the most appropriate page of a website, rather than to a home page or top level page.
===

Sixel
Sixel, short for "six pixels", is a bitmap graphics format supported by terminals and printers from [DEC]. It consists of a pattern six pixels high and one wide, resulting in 63 possible patterns. Each possible pattern is assigned an [ASCII] character, making the sixels easy to transmit on 7-bit serial links.

Sixel was first introduced as a way of sending bitmap graphics to DEC dot matrix printers like the LA50. After being put into "sixel mode" the following data was interpreted to directly control six of the pins in the nine pin print head. A string of sixel characters encodes a single 6 pixel high row of the image.

The system was later re-used as a way to send bitmap data to the [VT200] series terminals as a way to define custom character sets. A number of sixels were used to define each character. Starting with the [VT300] series, the terminals could decode a complete sixel image to the screen, like those previously sent to printers.

Sixel encodes images by breaking up the bitmap into a series of 6 pixel high horizontal strips. Each 1 pixel wide vertical column in a particular strip forms a single sixel. Each sixel's pixels are read as binary and encoded into a single 6 bit number, with "on" pixels encoded as a 1. This number, from 0 to 63 decimal, is then converted into a single ASCII character, offset by 63 so that an all-black sixel, 0 decimal, is encoded as ?. This ensures that the sixels remain within the easily printable range of the character set. Carriage return (CR) is represented by $, and line feeds (LF) with a -, both had to be sent in turn to return the cursor to the start of the line, CRLF.

Sixel also included a rudimentary form of compression, using Run Length Encoding ([RLE]). This was accomplished with the ! character followed by a decimal number of the times to repeat, and then a single sixel character to be repeated. Since the ! and decimal digits could not be valid sixel data, lying outside the encoded range, the encoding was easy to identify and expand back out in software.

"Sixel mode" was entered by sending the sequence <ESC>Pp1;p2;p3;q. The p1 through p3 were optional setup parameters, with p1 defining an aspect ratio (deprecated in favor of p3), p2 how to interpret the color of zeros, and p3 with simple grid size parameters. <ESC>P is the standard DEC "Device Control String", or DCS, which was used to turn on or off a number of special features in DEC's equipment. The "q" is the sixel identifier. Sixel data then followed the q. The "Stop Text" sequence <ESC>\ returned the device back to normal character mode again.

For printing, sixels are sent to the printer, decoded back into binary, and sent directly to the six pins of the print head. The only complexity involved expanding the RLE's into the internal print buffer. Display on a terminal is somewhat more difficult. On terminals supporting graphics, the [ReGIS] graphics system was used to directly draw the sixel pattern into the screen's bitmap. This was done at high speed by storing the bitmap patterns as a glyph and then bitting them.

When used for defining custom character sets the format was almost identical, although the escape codes changed. In terms of the data, the only major difference was the replacement of the separate CR/LF with a single /. In the VT300 series for instance, 80-column character glyphs were 15 pixels wide by 12 high, meaning that a character could be defined by sending a total of 30 sixels.

Colour was also supported using the # character, followed by a number referring to one of a number of colour registers, which varied from device to device. Colours were defined using either [RGB] or [HSV] values in a peculiar DEC format. On a printer, a line of sixels was sent several times, each representing a single bitplane from the register based colours on the terminals (normally 2 or 4 bits). For non-graphics terminals, colour was simply dropped. Since the capabilities of the hardware varied widely, a colour sixel drawing could only be output to targeted devices.
===

Run Length Encoding (RLE) 
Run Length Encoding (RLE) is a very simple form of data compression in which runs of data (that is, sequences in which the same data value occurs in many consecutive data elements) are stored as a single data value and count, rather than as the original run. This is most useful on data that contains many such runs. Consider, for example, simple graphic images such as icons, line drawings, and animations. It is not useful with files that don't have many runs as it could greatly increase the file size.

RLE may also be used to refer to an early graphics file format supported by [CompuServe] for compressing black and white images, but was widely supplanted by their later [Graphics Interchange Format]. RLE also refers to a little used image format in [Windows 3.x], with the extension .rle, which is a Run Length Encoded Bitmap, used to compress the Windows 3.x startup screen.

Typical applications of this encoding are when the source information comprises long substrings of the same character or binary digit.
===

CompuServe
CompuServe (CompuServe Information Service, also known by its acronym CIS) was the first major commercial online service in the United States. It dominated the field during the 1980s and remained a major player through the mid 1990s, when it was sidelined by the rise of services such as [AOL] with monthly subscriptions rather than hourly rates. Since the purchase of CompuServe's Information Services Division by AOL, it has operated as an online service provider and an Internet service provider. The original CompuServe Information Service, later rebranded as CompuServe Classic, was shut down 1 July 2009. The newer version of the service, CompuServe 2000, continues to operate as an internet portal.
===

NSI Escape Codes
In computing, ANSI escape codes (or escape sequences) are a method using in-band signaling to control the formatting, colour, and other output options on video text terminals. To encode this formatting information, certain sequences of bytes are embedded into the text, which the terminal looks for and interprets as commands, not as character codes.

ANSI codes were introduced in the 1970s and became widespread in the minicomputer / mainframe market by the early 1980s. They were used by the nascent [bulletin board system] market to offer improved displays compared to earlier systems lacking cursor movement, leading to even more widespread use.

Although hardware text terminals have become increasingly rare in the 21st century, the relevance of the ANSI standard persists because most terminal emulators interpret at least some of the ANSI escape sequences in the output text. One notable exception is the win32 console component of [Microsoft Windows].
===

Mac Terminal
Terminal (Terminal.app) is the terminal emulator included in the [OS X] operating system by [Apple].

Terminal originated in [NeXTSTEP] and OPENSTEP, the predecessor operating systems of OS X.

As a terminal emulator, the application provides text-based access to the operating system, in contrast to the mostly graphical nature of the user experience of OS X, by providing a command line interface to the operating system when used in conjunction with a [Unix] shell, such as [bash].

The preferences dialog for Terminal.app in OS X 10.8 ([Mountain Lion]) offers choices for values of the TERM environment variable. Available options are ansi, dtterm, nsterm, rxvt, vt52, vt100, vt102, xterm, xterm-16color and xterm-256color, which differ from the OS X 10.5 ([Leopard]) choices by dropping the xterm-color and adding xterm-16color and xterm-256color. These settings do not alter the operation of Terminal, and the xterm settings do not match the behaviour of xterm.

Terminal includes several features that specifically access OS X APIs and features. These include the ability to use the standard OS X Help search function to find manual pages and integration with Spotlight. Terminal was used by Apple as a showcase for OS X graphics APIs in early advertising of Mac OS X, offering a range of custom font and colouring options, including transparent backgrounds.
===

Wyse
Wyse is a leading manufacturer of cloud computing systems. They are best known for their video terminal line introduced in the 1980s, which competed with the market leading [DEC]. They also had a successful line of [IBM PC compatible] in the mid to late 1980s, but were outcompeted by companies such as [Dell] starting late in the decade. Current products include thin client hardware and software as well as desktop virtualisation. Other products include cloud software supporting desktop computers, laptops, and mobile devices.

On 2 April 2012, Dell and Wyse announced that Dell intends to take over the company. With this acquisition Dell would surpass their rival [Hewlett-Packard] in the market for thin clients. On 25 May 2012 Dell informed the market that it had completed the acquisition of Wyse Technology, which is now known as Dell Wyse.
===

National Replacement Character Set
The National Replacement Character Set, or NRCS for short, was a feature supported by later models of Digital's ([DEC]) computer terminal systems, starting with the [VT200] series in 1983. NRCS allowed individual characters from one character set to be replaced by one from another set, allowing the construction of different character sets on the fly. It was used to customize the character set to different local languages, without having to change the terminal's [ROM] for different counties, or alternately, include many different sets in a larger ROM. Many 3rd party terminals and terminal emulators supporting VT200 codes also supported NRCS.

[ASCII] is a 7-bit standard, allowing a total of 128 characters in the character set. Some of these are reserved as control characters, leaving 96 printable characters. This set of 96 includes upper and lower case letters, numbers, and basic math and punctuation.

ASCII does not have enough room to include other common characters such as multi-national currency symbols or the various accented letters common in European languages. This led to a number of country specific varieties of 7-bit ASCII with certain characters replaced. For instance, the UK standard simply replaced ASCII's hash mark, #, with the pound symbol, £. This normally led to different models of a given computer terminal or printer, differing only in the [glyphs] stored in ROM. These were standardised as part of ISO/IEC 646.

On an [8-bit clean] serial link, ASCII can be expanded to support a total of 256 characters. In this case, instead of replacing the characters in the original printable characters range from 32 to 127, new characters are added in the 128 to 255 range. This offers enough room for a single character set to include all the variety of characters used in North America and western Europe. This capability led to the introduction of the ISO/IEC 8859-1 standard character set containing 191 characters of what it calls the "Latin alphabet no. 1", but normally referred to as "ISO Latin". [Windows-1252] is a slightly expanded superset of ISO Latin.

NRCS was introduced to solve the problem of requiring different terminals for each country by allowing characters in the basic 7-bit ASCII set to be re-defined by copying the glyph from the DEC's version of ISO Latin, the [Multinational Character Set] (MCS). This meant that the ROM had to store only two character sets, standard ASCII and MCS, and could build any required local ASCII variant on the fly. For instance, instead of having a separate "UK ASCII" version of the terminal with a modified glyph in ROM, the terminal included an NRCS with instructions to replace the hash mark glyph with the pound. When used in the UK, typing Shift 3 produced the pound, the same keys pressed on a US terminal produced hash.

The NRCS could be set through a setup command, or more commonly, by replacing the keyboard with a model that sent back a code when first booted. That way simply plugging in a UK keyboard, which had a pound sign on the 3 key, automatically set the NRSC to that same replacement.
===

Multinational Character Set (MCS)
The Multinational Character Set (MCS) is a character encoding created by [Digital Equipment Corporation] for use in the popular [VT220] terminal. It was an [8-bit] extension of ASCII that added accented characters, currency symbols, and other character [glyphs] missing from 7-bit [ASCII]. It is only one of the code pages implemented for the VT220 [National Replacement Character Set].

Such "[extended ASCII]" sets were common (the National Replacement Character Set provided sets for more than a dozen European languages), but MCS has the distinction of being the ancestor of both [ISO 8859-1] and [Unicode].
===

Extended ASCII
Extended ASCII (or high ASCII) is [8-bit] or larger character encodings that include the standard 7-bit [ASCII] characters as well as others. The use of the term is sometimes criticised, because it can be mistakenly interpreted that the ASCII standard has been updated to include more than 128 characters or that the term unambiguously identifies a single encoding, both of which are untrue.
===

Glyph
In typography, a glyph is an elemental symbol within an agreed set of symbols, intended to represent a readable character for the purposes of writing and thereby expressing thoughts, ideas and concepts. As such, glyphs are considered to be unique marks that collectively add up to the spelling of a word, or otherwise contribute to a specific meaning of what is written, with that meaning dependent on cultural and social usage.

For example, in most languages written in any variety of the Latin alphabet the dot on a lower-case i is not a glyph because it does not convey any distinction, and an i in which the dot has been accidentally omitted is still likely to be recognised correctly. In Turkish, however, it is a glyph because that language has two distinct versions of the letter i, with and without a dot.

In Japanese syllabaries, a number of the characters are made up of more than one separate mark, but in general these separate marks are not glyphs because they have no meaning by themselves. However, in some cases, additional marks fulfil the role of diacritics, to differentiate distinct characters. Such additional marks constitute glyphs.

In general, a diacritic is a glyph, even if (like a cedilla in French, the ogonek in several languages or the stroke on a Polish "Ł") it is "joined up" with the rest of the character.

Some characters, such as "æ" in Icelandic and the "ß" in German, would probably be regarded as glyphs: they were originally ligatures but over time have become characters in their own right, and these languages treat them as separate letters. However, a ligature such as "ſi", which is treated in some typefaces as a single unit, is arguably not a glyph as this is just a quirk of the typeface, essentially an allographic feature, and includes more than one grapheme. In normal handwriting, even long words are often written "joined up", without the pen leaving the paper, and the form of each written letter will often vary depending on which letters precede and follow it, but that does not make the whole word into a single glyph.

Two or more glyphs which have the same significance, whether used interchangeably or chosen depending on context, are called allographs of each other.
===

8-bit Clean
8-bit clean describes a computer system that correctly handles [8-bit] character encodings, such as the [ISO 8859] series and the [UTF-8] encoding of [Unicode].

Up to the early 1990s, many programs and data transmission channels assumed that all characters would be represented as numbers between 0 and 127 (7 bits). On computers and data links using 8-bit bytes this left the top bit of each byte free for use as a parity, flag bit, or meta data control bit. 7-bit systems and data links are unable to handle more complex character codes which are commonplace in non-English speaking countries with larger alphabets.

Binary files cannot be transmitted through 7-bit data channels directly. To work around this, binary-to-text encodings have been devised which use only 7-bit [ASCII] characters. Some of these encodings are uuencoding, Ascii85, SREC, BinHex, kermit and [MIME]'s [Base64]. EBCDIC-based systems cannot handle all characters used in UUencoded data. However, the base64 encoding does not have this problem.
===

RS-232
In telecommunications, RS-232 is a standard for serial communication transmission of data. It formally defines the signals connecting between a DTE (data terminal equipment) such as a computer terminal, and a DCE (data circuit-terminating equipment, originally defined as data communication equipment), such as a modem. The RS-232 standard is commonly used in computer serial ports. The standard defines the electrical characteristics and timing of signals, the meaning of signals, and the physical size and pinout of connectors. The current version of the standard is TIA-232-F Interface Between Data Terminal Equipment and Data Circuit-Terminating Equipment Employing Serial Binary Data Interchange, issued in 1997.

An RS-232 serial port was once a standard feature of a personal computer, used for connections to [modems], printers, mice, data storage, uninterruptible power supplies, and other peripheral devices. However, RS-232 is hampered by low transmission speed, large voltage swing, and large standard connectors. In modern personal computers, [USB] has displaced RS-232 from most of its peripheral interface roles. Many computers do not come equipped with RS-232 ports and must use either an external USB-to-RS-232 converter or an internal expansion card with one or more serial ports to connect to RS-232 peripherals. Nevertheless, RS-232 devices are still used, especially in industrial machines, networking equipment and scientific instruments.
===

Centronics Data Computer Corporation
Centronics Data Computer Corporation was an American manufacturer of computer printers, now remembered primarily for the [parallel interface] that bears its name.

Centronics began as a division of [Wang Laboratories]. Founded and initially operated by Robert Howard (president) and Samuel Lang (vice president and owner of the well known K & L Color Photo Service Lab in New York City), the group produced remote terminals and systems for the casino industry. Printers were developed to print receipts and transaction reports. Wang spun off the business in 1971 and Centronics was formed as a corporation in Hudson, New Hampshire with Howard as president and chairman.

The Centronics Model 101 was introduced at the 1970 National Computer Conference. The print head used an innovative seven-wire solenoid impact system. Based on this design, Centronics later made the claim to have developed the first dot matrix impact printer.

Howard developed a personal relationship with his neighbour, Max Hugel, the founder and president of [Brother International], the United States arm of Brother Industries, Ltd., a manufacturer of sewing machines and typewriters. A business relationship developed when Centronics needed reliable manufacturing of the printer mechanisms a relationship that would help propel Brother into the printer industry. Hugel would later become executive vice president of Centronics. Print heads and electronics were built in Centronics plants in New Hampshire and Ireland, mechanisms were built in Japan by Brother and the printers were assembled in New Hampshire.

In the 1970s, Centronics formed a relationship with Canon to develop non-impact printers. No products were ever produced, but Canon continued to work on laser printers, eventually developing a highly successful series of engines.

In 1977, Centronics sued competitor Mannesmann AG in a patent dispute regarding the return spring used in the print actuator.

In 1975, Centronics formed an [OEM] agreement with [Tandy] and produced DMP and LP series printers for several years. The 6000 series band printers were introduced in 1978. By 1979 company revenues were over $100 million.

In 1980, the Mini-Printer Model 770 was introduced a small, low-cost desktop serial matrix printer. This was the first printer built completely in-house, and there were problems. Flaws in the microprocessor led to a recall and a stoppage of manufacturing for a year. During this period, Epson, Brother and others began to gain market share and Centronics never recovered. 1980 also saw the introduction of the E Series 900 and 1200 LPM band printers.
===

Dot Matrix Printing
Dot matrix printing or impact matrix printing is a type of computer printing which uses a print head that moves back and forth, or in an up and down motion, on the page and prints by impact, striking an ink soaked cloth ribbon against the paper, much like the print mechanism on a typewriter. However, unlike a typewriter or daisy wheel printer, letters are drawn out of a dot matrix, and thus, varied fonts and arbitrary graphics can be produced.

In the 1970s and 1980s, dot matrix impact printers were generally considered the best combination of expense and versatility, and until the 1990s they were by far the most common form of printer used with personal and home computers.

The [Epson] MX-80, introduced in 1979, was the groundbreaking model that sparked the initial popularity of impact printers in the personal computer market. The MX-80 combined affordability with good quality text output (for its time). Early impact printers (including the MX) were notoriously loud during operation, a result of the hammer like mechanism in the print head. The MX-80 even inspired the name of a noise rock band. The MX-80's low dot density (60 dpi horizontal, 72 dpi vertical) produced printouts of a distinctive "computerised" quality. When compared to the crisp typewriter quality of a daisy-wheel printer, the dot-matrix printer's legibility appeared especially bad. In office applications, output quality was a serious issue, as the dot-matrix text's readability would rapidly degrade with each photocopy generation. [IBM] sold the MX-80 as IBM 5125.
===

Seiko Epson Corporation
Seiko Epson Corporation commonly known as Epson, is a Japanese electronics company and one of the world's largest manufacturers of computer printers, and information and imaging related equipment. Headquartered in Suwa, Nagano, Japan, the company has numerous subsidiaries worldwide and manufactures inkjet, dot matrix and laser printers, scanners, desktop computers, business, multimedia and home theatre projectors, large home theatre televisions, robots and industrial automation equipment, point of sale docket printers and cash registers, laptops, integrated circuits, LCD components and other associated electronic components. It is one of three core companies of the Seiko Group, a name traditionally known for manufacturing Seiko timepieces since its founding.
===

Brother Industries Ltd
Brother Industries Ltd. is a Japanese multinational electronics and electrical equipment company headquartered in Nagoya, Japan. Its products include printers, multifunction printers, sewing machines, large machine tools, label printers, typewriters, fax machines, and other computer-related electronics. Brother distributes its products both under its own name and under [OEM] agreements with other companies.
===

Wang Laboratories
Wang Laboratories was a computer company founded in 1951 by Dr. An Wang and Dr. G. Y. Chu. The company was successively headquartered in Cambridge, Massachusetts (1954 – 1963), Tewksbury, Massachusetts (1963 – 1976), and finally in Lowell, Massachusetts (1976 – 1997). At its peak in the 1980s, Wang Laboratories had annual revenues of $3 billion and employed over 33,000 people.

The company was directed by Dr. Wang, who was described as an "indispensable leader" and played a personal role in setting business and product strategy until his death in 1990. Under his direction, the company went through several distinct transitions between different product lines.

Wang Laboratories filed for bankruptcy protection in August 1992. After emerging from bankruptcy, the company eventually changed its name to Wang Global. Wang Global was acquired by Getronics of The Netherlands in 1999, becoming Getronics North America, then was sold to KPN in 2007 and CompuCom in 2008, after which it no longer existed as a distinct brand or division.
===

Tandy
Tandy Leather, which later grew in to the Tandy Corporation, was a family owned leather goods company based in Fort Worth, Texas. Tandy Leather was founded in 1919 as a leather supply store, and acquired a number of craft retail companies, including RadioShack in 1963. In 2000, the Tandy Corporation name was dropped and entity became the RadioShack Corporation, selling The Tandy Leather name and operating assets to The Leather Factory.
===

Canon
Canon Inc is a Japanese multinational corporation specialised in the manufacture of imaging and optical products, including cameras, camcorders, photocopiers, steppers, computer printers and medical equipment. Its headquarters are located in Ōta, Tokyo, Japan.

Canon has a primary listing on the Tokyo Stock Exchange and is a constituent of the TOPIX index. It has a secondary listing on the New York Stock Exchange. At the beginning of 2015, Canon was the tenth largest public company in Japan when measured by market capitalisation.
===

Parallel Port
A parallel port is a type of interface found on computers (personal and otherwise) for connecting peripherals. In computing, a parallel port is a parallel communication physical interface. It is also known as a printer port or Centronics port. It was an industry de facto standard for many years, and was finally standardised as IEEE 1284 in the late 1990s, which defined the Enhanced Parallel Port (EPP) and Extended Capability Port (ECP) bi-directional versions. Today, the parallel port interface is seeing decreasing use because of the rise of Universal Serial Bus ([USB]) devices, along with network printing using [Ethernet].

The parallel port interface was originally known as the Parallel Printer Adapter on [IBM PC compatible] computers. It was primarily designed to operate a line printer that used [IBM]'s [8-bit] extended [ASCII] character set to print text, but could also be used to adapt other peripherals. Graphical printers, along with a host of other devices, have been designed to communicate with the system.

The term "Centronics port" now commonly refers to an IEEE-1284 Type B or 36-pin micro ribbon interface. The first parallel interface for printers was introduced with the [Centronics] Model 101 printer in 1970. The interface was developed by Dr. An Wang, Robert Howard and Prentice Robinson at Centronics. [Wang] had a surplus stock of 20,000 Amphenol 36-pin micro ribbon connectors that were originally used for one of their early calculators, which they used to create the Centronics interface on their computers. The connector has become so closely associated with Centronics that it is now popularly known as the “Centronics connector”.

The Centronics parallel interface quickly became an industry de facto standard; manufacturers of the time tended to use various connectors on the system side, so a variety of cables were required. For example, early [VAX] systems used a DC-37 connector, NCR used the 36-pin micro ribbon connector, [Texas Instruments] used a 25-pin card edge connector and [Data General] used a 50-pin micro ribbon connector.

When IBM implemented the parallel interface on the IBM PC, they used the DB-25F connector at the PC-end of the interface, creating the now familiar parallel cable with a DB25M at one end and a 36 pin micro ribbon connector at the other. [HP] adopted Centronics parallel on their printer models and introduced a bidirectional version known as Bitronics on the LaserJet 4 in 1992. The Bitronics and Centronics interfaces were superseded by the IEEE 1284 standard in 1994.

Centronics parallel is generally compliant with IEEE 1284 compatibility mode. The original Centronics implementation called for the busy lead to toggle with each received line of data (busy by line), whereas IEEE 1284 calls for busy to toggle with each received character (busy by character). Some host systems or print servers may use a strobe signal with a relatively low voltage output or a fast toggle. Any of these issues might cause no or intermittent printing, missing or repeated characters or garbage printing. Some printer models may have a switch or setting to set busy by character; others may require a handshake adapter.
===

Texas Instruments
Texas Instruments Inc is an American electronics company that designs and makes semiconductors, which it sells to electronics designers and manufacturers globally. Headquartered at Dallas, Texas, United States, TI is the third largest manufacturer of semiconductors worldwide after [Intel] and [Samsung], the second largest supplier of chips for cellular handsets after Qualcomm, and the largest producer of digital signal processors (DSPs) and analog semiconductors, among a wide range of other semiconductor products, including calculators, microcontrollers and multi-core processors. Texas Instruments is among the Top 20 Semiconductor producing companies in the world.

Texas Instruments was founded in 1951. It emerged after a reorganisation of Geophysical Service. This company manufactured equipment for use in the seismic industry as well as defence electronics. TI began research in transistors in the early 1950s and produced the world's first commercial silicon transistor. In 1954, Texas Instruments designed and manufactured the first transistor radio and Jack Kilby invented the integrated circuit in 1958 while working at TI's Central Research Labs. The company produced the first integrated circuit-based computer for the U.S. Air Force in 1961. TI researched infrared technology in the late 1950s and later made radar systems as well as guidance and control systems for both missiles and bombs. The hand-held calculator was introduced to the world by TI in 1967.

In the 1970s and 80s the company focused on consumer electronics including digital clocks, watches, hand-held calculators, home computers as well as various sensors. In 1997, its defence business was sold to Raytheon. In 2007, Texas Instruments was awarded the Manufacturer of the Year for Global Supply Chain Excellence by World Trade magazine. Texas Instruments is considered to be one of the most ethical companies in the world.

After the acquisition of National Semiconductor in 2011, the company has a combined portfolio of nearly 45,000 analog products and customer design tools, making it the world's largest maker of analog technology components. In 2011, Texas Instruments ranked 175 in the Fortune 500. TI is made up of two main divisions: Semiconductors (SC) and Educational Technology (ET) of which Semiconductor products account for approximately 96% of TI's revenue.
===

National Semiconductor
National Semiconductor was an American semiconductor manufacturer which specialised in analog devices and subsystems, formerly headquartered in Santa Clara, California, United States. The company produced power management integrated circuits, display drivers, audio and operational amplifiers, communication interface products and data conversion solutions. National's key markets included wireless handsets, displays and a variety of broad electronics markets, including medical, automotive, industrial and test and measurement applications.

On 23 September 2011, the company formally became part of [Texas Instruments] as the "[Silicon Valley]" division.
===

Silicon Valley
Silicon Valley is a nickname for the southern portion of the San Francisco Bay Area in California, United States. It is home to many of the world's largest high-tech corporations, as well as thousands of tech startup companies. The region occupies roughly the same area as the Santa Clara Valley where it is centered, including San Jose and surrounding cities and towns. The term originally referred to the large number of silicon chip innovators and manufacturers in the region, but eventually came to refer to all high tech businesses in the area, and is now generally used as a metonym for the American high-technology economic sector.

Silicon Valley is a leading hub and startup ecosystem for high-tech innovation and development, accounting for one-third of all of the venture capital investment in the United States. Geographically, Silicon Valley is generally thought to encompass all of the Santa Clara Valley, San Francisco, the San Francisco Peninsula, and southern portions of the East Bay.
===

Data General 
Data General was one of the first minicomputer firms from the late 1960s. Three of the four founders were former employees of [Digital Equipment Corporation]. Their first product, the Data General Nova, was a 16-bit minicomputer. This used their own operating system, RDOS, and in conjunction with programming languages like "Data General Business Basic" they provided a multi-user operating system with record locking and built-in databases far ahead of many contemporary systems. The Nova was followed by the Supernova and Eclipse product lines, all of which were used in many applications for the next two decades. The company employed an Original Equipment Manufacturer ([OEM]) sales strategy to sell to third parties who incorporated Data General computers into the OEM's specific product lines. A series of missteps in the 1980s, including missing the advance of microcomputers despite the launch of the microNOVA in 1977, and the Data General-One portable computer in 1984, led to a decline in the company's market share. The company did continue into the 1990s, however, and was eventually acquired by EMC Corporation in 1999.
===

USB - Universal Serial Bus
USB, short for Universal Serial Bus, is an industry standard developed in the mid 1990s that defines the cables, connectors and communications protocols used in a bus for connection, communication, and power supply between computers and electronic devices.

USB was designed to standardise the connection of computer peripherals (including keyboards, pointing devices, digital cameras, printers, portable media players, disk drives and network adapters) to personal computers, both to communicate and to supply electric power. It has become commonplace on other devices, such as smartphones, PDAs and video game consoles. USB has effectively replaced a variety of earlier interfaces, such as serial and parallel ports, as well as separate power chargers for portable devices.

Version History
Release name				Release date
USB 0.8						December 1994		
USB 0.9						April 1995		
USB 0.99					August 1995		
USB 1.0 Release Candidate	November 1995		
USB 1.0						January 1996
USB 1.1						August 1998		
USB 2.0						April 2000
USB 3.0						November 2008
USB 3.1						July 2013

USB 1.x
Released in January 1996, USB 1.0 specified data rates of 1.5 Mbit/s (Low Bandwidth or Low Speed) and 12 Mbit/s (Full Bandwidth or Full Speed). It did not allow for extension cables or pass-through monitors, due to timing and power limitations. Few USB devices made it to the market until USB 1.1 was released in August 1998, fixing problems identified in 1.0, mostly related to using hubs. USB 1.1 was the earliest revision that was widely adopted.

USB 2.0
USB 2.0 was released in April 2000, adding a higher maximum signalling rate of 480 Mbit/s called High Speed, in addition to the USB 1.x Full Speed signalling rate of 12 Mbit/s. Due to bus access constraints, the effective throughput of the High Speed signalling rate is limited to 35 MB/s or 280 Mbit/s.

Further modifications to the USB specification have been made via Engineering Change Notices (ECN). The most important of these ECNs are included into the USB 2.0 specification package available from USB.org.

- Mini-A and Mini-B Connector ECN: Released in October 2000.
	- Specifications for mini-A and B plug and receptacle. Also receptacle that accepts both plugs for On-The-Go. These should not be confused with micro-B plug and receptacle.
- Pull-up/Pull-down Resistors ECN: Released in May 2002
	- Interface Associations ECN: Released in May 2003.
	- New standard descriptor was added that allows associating multiple interfaces with a single device function.
- Rounded Chamfer ECN: Released in October 2003.
	- A recommended, backward compatible change to mini-B plugs that results in longer lasting connectors.
- Unicode ECN: Released in February 2005.
	- This ECN specifies that strings are encoded using UTF-16LE. USB 2.0 specified Unicode, but did not specify the encoding.
- Inter-Chip USB Supplement: Released in March 2006
- On-The-Go Supplement 1.3: Released in December 2006.
	- USB On-The-Go makes it possible for two USB devices to communicate with each other without requiring a separate USB host. In practice, one of the USB devices acts as a host for the other device.
- Battery Charging Specification 1.1: Released in March 2007 and updated on 15 April 2009.
	- Adds support for dedicated chargers (power supplies with USB connectors), host chargers (USB hosts that can act as chargers) and the No Dead Battery provision, which allows devices to temporarily draw 100 mA current after they have been attached. If a USB device is connected to dedicated charger, maximum current drawn by the device may be as high as 1.8 A. (Note that this document is not distributed with USB 2.0 specification package, only USB 3.0 and USB On-The-Go.)
- Micro-USB Cables and Connectors Specification 1.01: Released in April 2007.
- Link Power Management Addendum ECN: Released in July 2007.
	- This adds "sleep", a new power state between enabled and suspended states. Device in this state is not required to reduce its power consumption. However, switching between enabled and sleep states is much faster than switching between enabled and suspended states, which allows devices to sleep while idle.
- Battery Charging Specification 1.2 Released in December 2010.
	- Several changes and increasing limits including allowing 1.5 A on charging ports for unconfigured devices, allowing High Speed communication while having a current up to 1.5 A and allowing a maximum current of 5 A.

USB 3.0
USB 3.0 standard was released in November 2008, defining a new SuperSpeed mode. A USB 3.0 port, usually coloured blue, is backward compatible with USB 2.0 devices and cables.

The USB 3.0 Promoter Group announced on 17 November 2008 that the specification of version 3.0 had been completed and had made the transition to the USB Implementers Forum (USB-IF), the managing body of USB specifications. This move effectively opened the specification to hardware developers for implementation in products.

The new SuperSpeed bus provides a fourth transfer mode with a data signalling rate of 5.0 Gbit/s, in addition to the modes supported by earlier versions. The payload throughput is 4 Gbit/s (due to the overhead induced by used 8b/10b encoding), and the specification considers it reasonable to achieve around 3.2 Gbit/s (0.4 GB/s or 400 MB/s), which should increase with future hardware advances. Communication is full-duplex in SuperSpeed transfer mode; in the modes supported previously, by 1.x and 2.0, communication is half-duplex, with direction controlled by the host.

As with previous USB versions, USB 3.0 ports come in low-power and high-power variants, providing 150 mA and 900 mA respectively, while simultaneously transmitting data at SuperSpeed rates. Additionally, there is a Battery Charging Specification (Version 1.2 – December 2010), which increases the power handling capability to 1.5 A but does not allow concurrent data transmission. The Battery Charging Specification requires that the physical ports themselves be capable of handling 5 A of current but limits the maximum current drawn to 1.5 A.

USB 3.1
A January 2013 press release from the USB group revealed plans to update USB 3.0 to 10 Gbit/s. The group ended up creating a new USB version, USB 3.1, which was released on 31 July 2013, introducing a faster transfer mode called "SuperSpeed USB 10 Gbit/s", putting it on par with a single first-generation Thunderbolt channel. The new mode's logo features a "Superspeed+" caption (stylised as SUPERSPEED+). The USB 3.1 standard increases the data signalling rate to 10 Gbit/s in the USB 3.1 Gen2 mode, double that of USB 3.0 (referred to as USB 3.1 Gen1) and reduces line encoding overhead to just 3% by changing the encoding scheme to 128b/132b. The first USB 3.1 implementation demonstrated transfer speeds of 7.2 Gbit/s.

The USB 3.1 standard is backward compatible with USB 3.0 and USB 2.0.

USB Type-C
Developed at roughly the same time as the USB 3.1 specification, but distinct from it, the USB Type-C Specification 1.0 defines a new small reversible plug connector for USB devices. The Type-C plug connects to both hosts and devices, replacing various Type-B and Type-A connectors and cables with a standard meant to be future proof, similar to [Apple] Lightning and Thunderbolt. The 24-pin double-sided connector provides four power / ground pairs, two differential pairs for USB 2.0 data bus (though only one pair is implemented in a Type-C cable), four pairs for high-speed data bus, two "sideband use" pins, and two configuration pins for cable orientation detection, dedicated biphase mark code (BMC) configuration data channel, and VCONN +5 V power for active cables. Type-A and Type-B adaptors and cables will be required for older devices in order to plug into Type-C hosts; adaptors and cables with a Type-C receptacle are not allowed.

Full-featured USB Type-C cables are active, electronically marked cables that contain a chip with an ID function based on the configuration data channel and vendor defined messages (VDMs) from the USB Power Delivery 2.0 specification. USB Type-C devices also support power currents of 1.5 A and 3.0 A over the 5 V power bus in addition to baseline 900 mA; devices can either negotiate increased USB current through the configuration line, or they can support the full Power Delivery specification using both BMC-coded configuration line and legacy BFSK-coded VBUS line.

Alternate Mode dedicates some of the physical wires in the Type-C cable for direct device-to-host transmission of alternate data protocols. The four high-speed lanes, two sideband pins, and—​for dock, detachable device and permanent cable applications only two USB 2.0 pins and one configuration pin can be used for Alternate Mode transmission. The modes are configured using VDMs through the configuration channel.
===

Sound Blaster
The Sound Blaster family of sound cards was the de facto standard for consumer audio on the [IBM PC compatible] system platform, until the widespread transition to [Microsoft Windows 95], which standardised the programming interface at application level (eliminating the importance of backward compatibility with Sound Blaster), and the evolution in PC design led to onboard motherboard-audio, which commoditised PC audio functionality.

The creator of Sound Blaster is the Singapore based firm [Creative Technology Limited], also known by the name of its United States subsidiary, Creative Labs.

Sound Blaster 16
The Sound Blaster 16, announced in June 1992, introduced:

- 16-bit CD-quality digital audio sampling;
- An MPU-401 compatible UART;
- A socket for the optional Advanced Signal Processor or Creative Signal Processor chip (ASP or later CSP); and
- A connector for the Wave Blaster, a 'Wavetable' daughterboard (sample-based synthesis).

The Sound Blaster 16 retained the Yamaha OPL-3 for FM synthesis and a backward compatible programming interface, so most software titles written for the older Sound Blasters and Sound Blaster Pros would run without modification.

Eventually this design proved so popular that Creative made a [PCI] version of this card. Moving the card off the [ISA] bus, which was already approaching obsolescence, meant that no line for host controlled ISA DMA was available, as the PCI slot offers no such line. Instead, the card used PCI bus mastering to transfer data from the main memory to the D/A converters. Since existing [DOS] programs expected to be able to initiate host controlled ISA DMA for producing sound, backward compatibility with the older Sound Blaster cards for DOS programs required a software driver work around; since this work around necessarily depended on the virtual [8086] mode of the PC's CPU in order to catch and reroute accesses from the ISA DMA controller to the card itself, it failed for a number of DOS games that either were not fully compatible with this CPU mode or needed so much free conventional memory that they could not be loaded with the driver occupying part of this memory. In [Microsoft Windows], there was no problem, as Creative's Windows driver software could handle both ISA and PCI cards correctly.

Sound Blaster AWE32
Released in March 1994, the Sound Blaster AWE32 (Advanced Wave Effects) introduced an all new MIDI synthesizer section based on the EMU8000. The AWE32 consisted of two distinct audio sections; the Creative digital audio section (audio codec, optional CSP/ASP chip socket, Yamaha OPL3), and the E-mu MIDI synthesizer section. The synthesizer section consisted of the EMU8000 sampler and effects processor, an EMU8011 1 MB sample ROM, and 512 KB of sample RAM (expandable to 28 MB). To fit the new hardware, the AWE32 was a full-length ISA card, measuring 14 in (360 mm).

Sound Blaster 32
A derivative of the AWE32 design, the Sound Blaster 32 (SB32) was a value-oriented offering from Creative. Announced on 6 June 1995, the SB32 became the new entry-level card in the AWE32 product-line (previously held by the AWE32 Value). The SB32 retained the AWE32's EMU8000/EMU8011 MIDI-synthesis engine and built-in instrument ROM, but dropped the onboard RAM, the Wave Blaster header, and the CSP port. The SB32 used the Vibra chip to reduce component count, which meant bass/treble/gain control was limited compared to the AWE32. The loss of onboard RAM is offset by the inclusion of 30-pin SIMM RAM sockets, which allow up to 8 MB RAM to be installed and used by the EMU engine.

Sound Blaster AWE64
The AWE32's successor, the Sound Blaster AWE64 (November 1996), was significantly smaller, being a "half-length ISA card". It offered similar features to the AWE32, but also had a few notable improvements, including support for greater polyphony, although this was a product of 32 extra software emulated channels. The 30-pin SIMM slots from AWE32/SB32 were replaced with a proprietary memory format which could be (expensively) purchased from Creative.

The main improvements were better compatibility with older SB models, and an improved signal-to-noise ratio. The AWE64 came in three versions: A Value version (with 512KB of RAM), a Standard version (with 1 MB of RAM), and a Gold version (with 4 MB of RAM and a separate S/PDIF output).
===

Industry Standard Architecture (ISA)
Industry Standard Architecture (ISA) is a retronym term for the [16-bit] internal bus of [IBM PC/AT] and similar computers based on the [Intel 80286] and its immediate successors during the 1980s. The bus was (largely) backward compatible with the [8-bit] bus of the [8088] based [IBM PC], including the [IBM PC/XT] as well as [IBM PC compatibles].

Originally referred to as the PC/AT-bus it was also termed I/O Channel by [IBM]. The ISA concept was coined by competing PC-clone manufacturers in the late 1980s or early 1990s as a reaction to IBM attempts to replace the AT-bus with its new and incompatible Micro Channel architecture.

The 16-bit ISA bus was used also with 32-bit processors for several years. An attempt to extend it to 32 bits, called Extended Industry Standard Architecture (EISA), was not very successful, however. Later buses such as [VESA Local Bus] and [PCI] were used instead, often along with ISA slots on the same mainboard. A derivative of the AT bus structure is still used in the PCMCIA standard, Compact Flash, the PC/104 bus, and internally within Super I/O chips.
===

Conventional PCI
Conventional PCI, often shortened to PCI, is a local computer bus for attaching hardware devices in a computer. PCI is the initialism for Peripheral Component Interconnect and is part of the PCI Local Bus standard. The PCI bus supports the functions found on a processor bus but in a standardised format that is independent of any particular processor's native bus. Devices connected to the PCI bus appear to a bus master to be connected directly to its own bus and are assigned addresses in the processor's address space. It is a parallel bus, synchronous to a single bus clock.

Attached devices can take either the form of an integrated circuit fitted onto the motherboard itself (called a planar device in the PCI specification) or an expansion card that fits into a slot. The PCI Local Bus was first implemented in [IBM PC compatibles], where it displaced the combination of several slow [ISA] slots and one fast [VESA Local Bus] slot as the bus configuration. It has subsequently been adopted for other computer types. Typical PCI cards used in PCs include: network cards, sound cards, modems, extra ports such as [USB] or serial, TV tuner cards and disk controllers. PCI video cards replaced ISA and VESA cards until growing bandwidth requirements outgrew the capabilities of PCI. The preferred interface for video cards then became AGP, itself a superset of conventional PCI, before giving way to PCI Express.

The first version of conventional PCI found in consumer desktop computers was a 32-bit bus using a 33 MHz bus clock and 5 V signalling, although the PCI 1.0 standard provided for a 64-bit variant as well. These have one locating notch in the card. Version 2.0 of the PCI standard introduced 3.3 V slots, physically distinguished by a flipped physical connector to preventing accidental insertion of 5 V cards. Universal cards, which can operate on either voltage, have two notches. Version 2.1 of the PCI standard introduced optional 66 MHz operation. A server-oriented variant of conventional PCI, called PCI-X (PCI Extended) operated at frequencies up to 133 MHz for PCI-X 1.0 and up to 533 MHz for PCI-X 2.0. An internal connector for laptop cards, called Mini PCI, was introduced in version 2.2 of the PCI specification. The PCI bus was also adopted for an external laptop connector standard—the CardBus. The first PCI specification was developed by Intel, but subsequent development of the standard became the responsibility of the PCI Special Interest Group (PCI-SIG).

Conventional PCI and PCI-X are sometimes called Parallel PCI in order to distinguish them technologically from their more recent successor PCI Express, which adopted a serial, lane-based architecture. Conventional PCI's heyday in the desktop computer market was approximately the decade 1995 - 2005. PCI and PCI-X have become obsolete for most purposes; however, they are still common on modern desktops for the purposes of backwards compatibility and the low relative cost to produce. Many kinds of devices previously available on PCI expansion cards are now commonly integrated onto motherboards or available in universal serial bus and PCI Express versions.
===

VESA Local Bus 
The VESA Local Bus (usually abbreviated to VL-Bus or VLB) was mostly used in personal computers. VESA (Video Electronics Standards Association) Local Bus worked alongside the [ISA] bus; it acted as a high-speed conduit for memory-mapped I/O and DMA, while the ISA bus handled interrupts and port-mapped I/O.
===

Creative Technology
Creative Technology Ltd is a Singapore based global company headquartered in Jurong East, Singapore. The principal activities of the company and its subsidiaries consist of the design, manufacture and distribution of digitised sound and video boards, computers and related multimedia, and personal digital entertainment products.

It also partners with mainboard manufacturers and laptop brands to embed its [Sound Blaster] technology on their products.
===

DOSBox
DOSBox is an emulator program that emulates an [IBM PC compatible] computer running a [DOS] operating system. Many IBM PC compatible graphics and sound cards are also emulated. This means that original DOS programs (including PC games) are provided an environment in which they can run correctly, even though the modern computers have dropped support for that old environment. DOSBox is free software written primarily in [C++] and distributed under the [GNU General Public License]. DOSBox has been downloaded over 25 million times since its release on SourceForge in 2002.

DOSBox can run old DOS software on modern computers which would not work otherwise, because of incompatibilities between the older software and modern hardware and operating systems.

A number of usability enhancements have been added to DOSBox beyond emulating DOS. The added features include virtual hard drives, peer-to-peer networking, screen capture and screencasting from the emulated screen.

The original DOSBox has not been updated in a long time. Active development is happening on forks of DOSBox. Forks such as SVN Daum and DOSBox-X provide additional features, which include support for save states and long filenames.

A number of vintage DOS games have been re-released by video game developers to run on modern computers by encapsulating them inside DOSBox.
===

Character Encoding
In computing, a character encoding is used to represent a repertoire of characters by some kind of an encoding system. Depending on the abstraction level and context, corresponding code points and the resulting code space may be regarded as bit patterns, octets, natural numbers, electrical pulses, etc. A character encoding is used in computation, data storage, and transmission of textual data. Terms such as character set, character map, codeset or code page are sometimes used as near synonyms; however, these terms have related but distinct meanings described in the article.

Early character codes associated with the optical or electrical telegraph could only represent a subset of the characters used in written languages, sometimes restricted to upper case letters, numerals and some punctuation only. The low cost of digital representation of data in modern computer systems allows more elaborate character codes (such as [Unicode]) which represent more of the characters used in many written languages. Character encoding using internationally accepted standards permits worldwide interchange of text in electronic form.
===

ISO/IEC 8859-1
ISO/IEC 8859-1 is a [8-bit] single-byte coded graphic character sets — Part 1: Latin alphabet No. 1, is part of the ISO/IEC 8859 series of [ASCII] based standard character encodings, first edition published in 1987. It is generally intended for Western European languages. It is the basis for most popular 8-bit character sets, including [Windows-1252] and the first block of characters in [Unicode].

ISO-8859-1 is the IANA preferred name for this standard when supplemented with the C0 and C1 control codes from ISO/IEC 6429. The following other aliases are registered for ISO-8859-1: iso-ir-100, csISOLatin1, latin1, l1, IBM819, CP819.

The Windows-1252 codepage coincides with ISO-8859-1 for all codes except the range 128 to 159 (hex 80 to 9F), where the little used C1 controls are replaced with additional characters including all the missing characters provided by ISO-8859-15. Code page 28591 a.k.a. Windows-28591 is the actual ISO-8859-1 codepage.
===

UTF-8
UTF-8 is a character encoding capable of encoding all possible characters, or code points, in [Unicode].

The encoding is variable length and uses [8-bit] code units. It was designed for backward compatibility with [ASCII], and to avoid the complications of endianness and byte order marks in the alternative [UTF-16] and [UTF-32] encodings. The name is derived from: Universal Coded Character Set + Transformation Format—8-bit.

UTF-8 is the dominant character encoding for the [World Wide Web], accounting for 84.3% of all Web pages in July 2015. The Internet Mail Consortium (IMC) recommends that all e-mail programs be able to display and create mail using UTF-8, and the [W3C] recommends UTF-8 as the default encoding in [XML] and [HTML].

UTF-8 encodes each of the 1,112,064 valid code points in the Unicode code space (1,114,112 code points minus 2,048 surrogate code points) using one to four 8-bit bytes (a group of 8 bits is known as an octet in the Unicode Standard). Code points with lower numerical values (i.e., earlier code positions in the Unicode character set, which tend to occur more frequently) are encoded using fewer bytes. The first 128 characters of Unicode, which correspond one-to-one with ASCII, are encoded using a single octet with the same binary value as ASCII, making valid ASCII text valid UTF-8-encoded Unicode as well. And ASCII bytes do not occur when encoding non-ASCII code points into UTF-8, making UTF-8 safe to use within most programming and document languages that interpret certain ASCII characters in a special way, e.g. as end of string.
===

UTF-16
UTF-16 (16-bit [Unicode] Transformation Format) is a character encoding capable of encoding all 1,112,064 possible characters in Unicode. The encoding is variable-length, as code points are encoded with one or two [16-bit] code units. (also see Comparison of Unicode encodings for a comparison of [UTF-8], UTF-16 & [UTF-32])

UTF-16 developed from an earlier fixed-width 16-bit encoding known as UCS-2 (for 2-byte Universal Character Set) once it became clear that a fixed-width 2-byte encoding could not encode enough characters to be truly universal.
===

UTF-32
UTF-32 (or UCS-4) stands for [Unicode] Transformation Format [32-bits]. It is a protocol to encode Unicode characters that uses exactly 32-bits per Unicode code point. This makes UTF-32 a fixed-length encoding, in contrast to all other Unicode transformation formats which are variable-length encodings. The UTF-32 form of a character is a direct representation of its codepoint.

The main advantage of UTF-32, versus variable-length encodings, is that the Unicode code points are directly indexable. Examining the n'th code point is a constant time operation. In contrast, a variable-length code requires sequential access to find the n'th code point. This makes UTF-32 a simple replacement in code that uses integers to index characters out of strings, as was commonly done for [ASCII].

The main disadvantage of UTF-32 is that it is space inefficient, using four bytes per character. Non-BMP characters are so rare in most texts, they may as well be considered non-existent for sizing issues, making UTF-32 up to twice the size of UTF-16 and up to four times the size of UTF-8.
===

Windows-1252
Windows-1252 or CP-1252 is a character encoding of the Latin alphabet, used by default in the legacy components of [Microsoft Windows] in English and some other Western languages. It is one version within the group of [Windows code pages].

This character encoding is a superset of [ISO 8859-1], but differs from the IANA's ISO-8859-1 by using displayable characters rather than control characters in the 80 to 9F (hex) range. Notable additional characters are curly quotation marks, the Euro sign, and all the printable characters that are in ISO 8859-15. It is known to Windows by the code page number 1252, and by the IANA-approved name "windows‑1252".

It is very common to mislabel Windows-1252 text with the charset label ISO-8859-1. A common result was that all the quotes and apostrophes (produced by "smart quotes" in word-processing software) were replaced with question marks or boxes on non-Windows operating systems, making text difficult to read. Most modern web browsers and e-mail clients treat the [MIME] charset ISO-8859-1 as Windows-1252 to accommodate such mislabeling. This is now standard behaviour in the [HTML 5] specification, which requires that documents advertised as ISO-8859-1 actually be parsed with the Windows-1252 encoding.

Historically, the phrase "ANSI Code Page" (ACP) is used in [Windows] to refer to various code pages considered as native. The intention was that most of these would be ANSI standards such as ISO-8859-1. Even though Windows-1252 was the first and by far most popular code page named so in Microsoft Windows parlance, the code page has never been an ANSI standard. Microsoft explains, "The term ANSI as used to signify Windows code pages is a historical reference, but is nowadays a misnomer that continues to persist in the Windows community."
===

Windows Code Pages
Windows code pages are sets of characters or code pages (known as character encodings in other operating systems) used in [Microsoft Windows] from the 1980s and 1990s. Windows code pages were gradually superseded when Unicode was implemented in Windows, although they are still supported both within Windows and other platforms.

There are two groups of code pages in Windows systems: OEM and ANSI code pages. Code pages in both of these groups are extended [ASCII] code pages.

ANSI code page
ANSI code pages (officially called "Windows code pages" after [Microsoft] accepted the former term being a misnomer) are used for native non-Unicode (say, byte oriented) applications using a graphical user interface on Windows systems. ANSI Windows code pages, and especially the code page 1252, were called that way since they were purportedly based on drafts submitted or intended for ANSI. However, ANSI and ISO have not standardised any of these code pages. Instead they are either supersets of the standard sets such as those of ISO 8859 and the various national standards (like Windows-1252 vs. ISO-8859-1), major modifications of these (making them incompatible to various degrees, like Windows-1250 vs. ISO-8859-2) or having no parallel encoding (like Windows-1257 vs. ISO-8859-4; ISO-8859-13 was introduced much later). About twelve of the typography and business characters from CP1252 at code points 0x80–0x9F (in ISO 8859 occupied by C1 control codes, which are useless in Windows) are present in many other ANSI / Windows code pages at the same codes. These code pages are labelled by Internet Assigned Numbers Authority ([IANA]) as "Windows-number".

OEM code page
The OEM code pages (original equipment manufacturer) are used by Win32 console applications, and by virtual DOS, and can be considered a holdover from [DOS] and the original [IBM PC] architecture. A separate suite of code pages was implemented not only due to compatibility, but also because the fonts of [VGA] (and descendant) hardware suggest encoding of line drawing characters to be compatible with code page 437. Most OEM code pages share many code points, particularly for non-letter characters, with the second (non-ASCII) half of CP437.

A typical OEM code page, in its second half, does not resemble any ANSI / Windows code page even roughly. Nevertheless, two single-byte, fixed-width code pages (874 for Thai and 1258 for Vietnamese) and four multibyte CJK code pages (932, 936, 949, 950) are used as both OEM and ANSI code pages. Code page 1258 uses combining diacritics, as Vietnamese requires more than 128 letter-diacritic combinations. This is in contrast to VISCII, which replaces some of the C0 (i.e. ASCII) control codes.

Windows Code Pages

ID		Names					Description						Type		Base				Encoding	Standard
37		CP037, IBM037			IBM EBCDIC US-Canada			Other		EBCDIC derivation	8-bit SBCS	IBM CP037	
437		CP437, IBM437			IBM PC US						OEM			ASCII derivation	8-bit SBCS	IBM CP437	
1250	CP1250, Windows-1250	Latin 2 / Central European		ANSI		ASCII derivation	8-bit SBCS	Microsoft CP1250	
1251	CP1251, Windows-1251	Cyrillic						ANSI		ASCII derivation	8-bit SBCS	Microsoft CP1251	
1252	CP1252, Windows-1252	Latin 1 / Western European		ANSI		ASCII derivation	8-bit SBCS	Microsoft CP1252
1253	CP1253, Windows-1253	Greek							ANSI		ASCII derivation	8-bit SBCS	Microsoft CP1253	
1254	CP1254, Windows-1254	Turkish							ANSI		ASCII derivation	8-bit SBCS	Microsoft CP1254		
1255	CP1255, Windows-1255	Hebrew							ANSI		ASCII derivation	8-bit SBCS	Microsoft CP1255	
1256	CP1256, Windows-1256	Arabic							ANSI		ASCII derivation	8-bit SBCS	Microsoft CP1256	
1257	CP1257, Windows-1257	Baltic							ANSI		ASCII derivation	8-bit SBCS	Microsoft CP1257		
1258	CP1258, Windows-1258	Vietnamese						OEM + ANSI	?					8-bit SBCS	Microsoft CP1258

====

Unicode
Unicode is a computing industry standard for the consistent encoding, representation, and handling of text expressed in most of the world's writing systems. Developed in conjunction with the Universal Character Set standard and published as The Unicode Standard, the latest version of Unicode contains a repertoire of more than 120,000 characters covering 129 modern and historic scripts, as well as multiple symbol sets. The standard consists of a set of code charts for visual reference, an encoding method and set of standard character encodings, a set of reference data files, and a number of related items, such as character properties, rules for normalization, decomposition, collation, rendering, and bidirectional display order (for the correct display of text containing both right-to-left scripts, such as Arabic and Hebrew, and left-to-right scripts).[1] As of June 2015, the most recent version is Unicode 8.0. The standard is maintained by the Unicode Consortium.

Unicode's success at unifying character sets has led to its widespread and predominant use in the internationalization and localization of computer software. The standard has been implemented in many recent technologies, including modern operating systems, XML, the Java programming language, and the Microsoft .NET Framework.

Unicode can be implemented by different character encodings. The most commonly used encodings are UTF-8, UTF-16 and the now-obsolete UCS-2. UTF-8 uses one byte for any ASCII character, all of which have the same code values in both UTF-8 and ASCII encoding, and up to four bytes for other characters. UCS-2 uses a 16-bit code unit (two 8-bit bytes) for each character but cannot encode every character in the current Unicode standard. UTF-16 extends UCS-2, using one 16-bit unit for the characters that were representable in UCS-2 and two 16-bit units (4 × 8 bit) to handle each of the additional characters.
===

Universal Coded Character Set (UCS)
The Universal Coded Character Set (UCS), defined by the International Standard ISO/IEC 10646, Information technology — Universal Coded Character Set (UCS) (plus amendments to that standard), is a standard set of characters upon which many character encodings are based. The UCS contains nearly one hundred thousand abstract characters, each identified by an unambiguous name and an integer number called its code point.

Characters (letters, numbers, symbols, ideograms, logograms, etc.) from the many languages, scripts, and traditions of the world are represented in the UCS with unique code points. The inclusiveness of the UCS is continually improving as characters from previously unrepresented writing systems are added.

Since 1991, the Unicode Consortium has worked with ISO to develop The Unicode Standard ("Unicode") and ISO/IEC 10646 in tandem. The repertoire, character names, and code points of Version 2.0 of Unicode exactly match those of ISO/IEC 10646-1 with its first seven published amendments. After the publication of [Unicode] 3.0 in February 2000, corresponding new and updated characters entered the UCS via ISO/IEC 10646-1. In 2003, parts 1 and 2 of ISO/IEC 10646 were combined into a single part, which has since had a number of amendments adding characters to the standard in approximate synchrony with the Unicode standard.

The UCS has over 1.1 million code points available for use, but only the first 65,536 (the Basic Multilingual Plane, or BMP) had entered into common use before 2000. This situation began changing when the People's Republic of China (PRC) ruled in 2000 that all software sold in its jurisdiction would have to support GB 18030. This required software intended for sale in the PRC to move beyond the BMP.

The system deliberately leaves many code points not assigned to characters, even in the BMP. It does this to allow for future expansion or to minimize conflicts with other encoding forms.
===

Web Server 
A Web server is an information technology that processes requests via [HTTP], the basic network protocol used to distribute information on the [World Wide Web]. The term can refer either to the entire computer system, an appliance, or specifically to the software that accepts and supervises the HTTP requests.

The primary function of a web server is to store, process and deliver web pages to clients. The communication between client and server takes place using the Hypertext Transfer Protocol (HTTP). Pages delivered are most frequently [HTML] documents, which may include images, style sheets and scripts in addition to text content.

A user agent, commonly a [Web browser] or Web crawler, initiates communication by making a request for a specific resource using HTTP and the server responds with the content of that resource or an error message if unable to do so. The resource is typically a real file on the server's secondary storage, but this is not necessarily the case and depends on how the web server is implemented.

While the primary function is to serve content, a full implementation of HTTP also includes ways of receiving content from clients. This feature is used for submitting web forms, including uploading of files.

Many generic web servers also support server-side scripting using Active Server Pages ([ASP]), [PHP], or other scripting languages. This means that the behaviour of the Web server can be scripted in separate files, while the actual server software remains unchanged. Usually, this function is used to generate HTML documents dynamically ("on-the-fly") as opposed to returning static documents. The former is primarily used for retrieving and / or modifying information from databases. The latter is typically much faster and more easily cached but cannot deliver dynamic content.

Web servers are not always used for serving the World Wide Web. They can also be found embedded in devices such as printers, routers, webcams and serving only a local network. The web server may then be used as a part of a system for monitoring and / or administering the device in question. This usually means that no additional software has to be installed on the client computer, since only a web browser is required (which now is included with most operating systems).

In 1989 [Tim Berners-Lee] proposed a new project to his employer [CERN], with the goal of easing the exchange of information between scientists by using a hypertext system. The project resulted in Berners-Lee writing two programs in 1990:

- A browser called [WorldWideWeb].
- The world's first web server, later known as [CERN httpd], which ran on [NeXTSTEP]

Between 1991 and 1994, the simplicity and effectiveness of early technologies used to surf and exchange data through the World Wide Web helped to port them to many different operating systems and spread their use among scientific organisations and universities, and then to industry.

In 1994 Tim Berners-Lee decided to constitute the World Wide Web Consortium ([W3C]) to regulate the further development of the many technologies involved (HTTP, HTML, etc.) through a standardisation process.
===

CERN Httpd
CERN httpd (later also known as W3C httpd) was a [Web server] ([HTTP]) [daemon] originally developed at [CERN] from 1990 onwards by [Tim Berners-Lee], Ari Luotonen and Henrik Frystyk Nielsen. Implemented in [C], it was the first ever Web server software and went live on Christmas Day 1990.

CERN httpd was originally developed on a [NeXT] Computer running [NeXTSTEP], and was later ported to other [Unix-like] operating systems and [VMS]. It could also be configured as a Web proxy server. Version 0.1 was released in June 1991. In August 1991, Berners-Lee announced in the Usenet newsgroup alt.hypertext the availability of the source code of the server daemon and other World Wide Web software from the CERN FTP site.

The original, first generation HTTP server which some call the Volkswagen of the Web.

The server was presented on the Hypertext 91 conference in San Antonio and was part of the CERN Program Library (CERNLIB).

Later versions of the server are based on the libwww library. The development of CERN httpd was later taken over by [W3C], with the last release being version 3.0A of 15 July 1996. From 1996 onwards, W3C focused on the development of the [Java] based [Jigsaw] server.

The initial version was public domain software; the last one was under an [MIT license].
===

Punched Card
A punched card, punch card, IBM card, or Hollerith card is a piece of stiff paper that contained either commands for controlling automated machinery or data for data processing applications. Both commands and data were represented by the presence or absence of holes in predefined positions.

Now obsolete as a recording medium, punched cards were widely used throughout the 20th century for controlling textile looms and in the late 19th and early 20th century for controlling fairground organs and related instruments. Punched cards were used through most of the 20th century in what became known as the data processing industry; the use of unit record machines, organised into data processing systems, for data input, processing, and storage. Early digital computers used punched cards, often prepared using keypunch machines, as the primary medium for input of both computer programs and data.
==

Punched Tape
Punched tape or perforated paper tape is a form of data storage, consisting of a long strip of paper in which holes are punched to store data. Now effectively obsolete, it was widely used during much of the twentieth century for teleprinter communication, for input to computers of the 1950s and 1960s, and later as a storage medium for minicomputers and [CNC] machine tools.

The earliest forms of punched tape come from weaving looms and embroidery, where cards with simple instructions about a machine's intended movements were first fed individually as instructions, then controlled by instruction cards, and later were fed as a string of connected cards.

This led to the concept of communicating data not as a stream of individual cards, but one "continuous card", or a tape. Many professional embroidery operations still refer to those individuals who create the designs and machine patterns as "punchers", even though punched cards and paper tape were eventually phased out, after many years of use, in the 1990s.

Tape Formats
Data were represented by the presence or absence of a hole at a particular location. Tapes originally had five rows of holes for data. Later tapes had 6, 7 and 8 rows. An early electro-mechanical calculating machine, the Automatic Sequence Controlled Calculator or Harvard Mark I, used paper tape with 24 rows. A row of narrower holes that were always punched served to feed the tape, originally using a wheel with radial teeth called a sprocket wheel. Later optical readers used the sprocket holes to generate timing pulses.

Text was encoded in several ways. The earliest standard character encoding was Baudot, which dates back to the nineteenth century and had 5 holes. The Baudot code was never used in teleprinters. Instead, modifications such as the Murray code (which added carriage return and line feed), Western Union code, International Telegraphic Alphabet #2 (ITA 2), and American Teletypewriter code (USTTY), were used. Other standards, such as Teletypesetter (TTS), Fieldata and Flexowriter, had 6 holes. In the early 1960s, the American Standards Association led a project to develop a universal code for data processing, which became known as [ASCII]. This 7-level code was adopted by some teleprinter users, including AT&amp;T (Teletype). Others, such as Telex, stayed with the earlier codes.

Dimensions
Tape for punching was 0.00394 inches (0.1 mm) thick. The two most common widths were 11/16 inch (17.46 mm) for five bit codes, and 1 inch (25.4 mm) for tapes with six or more bits. Hole spacing was 0.1 inch (2.54 mm) in both directions. Data holes were 0.072 inches (1.83 mm) in diameter; feed holes were 0.046 inches (1.17 mm). Paper tape rolls in both widths are still commercially available as of 2012.

Chad
Most tape-punching equipment used solid punches to create holes in the tape. This process inevitably created "chad", or small circular pieces of paper. Managing the disposal of chad was an annoying and complex problem, as the tiny paper pieces had a tendency to escape and interfere with the other electromechanical parts of the teleprinter equipment.
===

Zip drive
The Zip drive is a medium capacity removable [floppy disk] storage system that was introduced by [Iomega] in late 1994. Originally, Zip disks launched with capacities of 100MB, but later versions increased this to first 250MB and then 750MB.

The format became the most popular of the superfloppy products which filled a niche in the late 1990s portable storage market. However, it was never popular enough to replace the [3.5-inch floppy disk] nor could ever match the storage size available on rewritable CDs and later rewritable DVDs. USB flash drives ultimately proved to be the better rewritable storage medium among the general public due to the near-ubiquity of USB ports on personal computers and soon after because of the far greater storage sizes offered. Zip drives fell out of favor for mass portable storage during the early 2000s. The Zip brand later covered internal and external CD writers known as Zip-650 or Zip-CD, which had no relation to the Zip drive.
===

LS-120 SuperDisk
Announced in 1995, the "SuperDisk" marketed as the LS-120 drive, often seen with the brand names Matsushita (Panasonic) and Imation, had an initial capacity of 120MB using even higher density "LS-120" disks.

LS in this case stands for Laser Servo, which used a very low power superluminescent LED that generated light with a small focal spot. This allowed the drive to align its rotation to precisely the same point each time, allowing far more data to be written due to the absence of conventional magnetic alignment marks. The alignment was based on hard coded optical alignment marks, which meant that a complete format could be done. This worked very well at the time and as a result failures associated with magnetic fields wiping the ZIP drive alignment Z tracks were less of a problem.

It was upgraded (as the "LS-240") to 240MB. Not only could the drive read and write 1440kB disks, but the last versions of the drives could write 32MB onto a normal 1440kB disk. Unfortunately, popular opinion held the Super Disk disks to be quite unreliable, though no more so than the [Zip drives] and SyQuest Technology offerings of the same period and there were also many reported problems moving standard floppies between LS-120 drives and normal floppy drives. This belief, true or otherwise, crippled adoption. The [BIOS] of many motherboards even to this day supports LS-120 drives as boot options.

One suggested improvement to LS-240 was the addition of an optical reader based on low resolution B/W CCD technology, as this would be able to detect disk flaws before they could cause data loss and adjust write strategies accordingly. Also it would allow detection of damage caused by head misalignment before the data surface was compromised. Had this ever been implemented then it would have allowed the LS240 to store nearly 500MB of data. This strategy was later implemented on [HP] and [Epson] printers to allow photo quality printing on normal non-photo paper.

LS-120 compatible drives were available as options on many computers, including desktop and notebook computers from [Compaq Computer Corporation]. In the case of the Compaq notebooks, the LS-120 drive replaced the standard floppy drive in a multi-bay configuration.
===
LenovoEMC (formerly Iomega), sometimes styled lenovo EMC², is a producer of external, portable, and networked storage solutions. Established in the 1980s as Iomega, LenovoEMC has sold more than 410 million digital storage drives and disks. It was a public company, trading on the New York Stock Exchange under the symbol IOM beginning in 1983.[2] The ZIP Drive was Iomega's most notable product.[3]
===

The Disc Filing System (DFS) is a computer file system developed by Acorn Computers, initially as an add-on to the Eurocard-based Acorn System 2.

In 1981, the Education Departments of Western Australia and South Australia announced joint tenders calling for the supply of personal computers to their schools. Acorn's Australian computer distributor, Barson Computers, convinced Joint Managing Directors Hermann Hauser and Chris Curry to allow the soon to be released Acorn BBC Microcomputer to be offered with disk storage as part of the bundle. They agreed on condition that Barson adapted the Acorn DFS from the System 2 without assistance from Acorn as they had no resources available. This required some minor hardware and software changes to make the DFS compatible with the BBC Micro.

Barson won the tenders for both states, with the DFS fitted, a year ahead of the UK. It was this early initiative that resulted in the BBC Micro being more heavily focused on the education market in Australia, with very little penetration of the home computer market until the arrival of the Acorn Electron.

The DFS shipped as a ROM and Disk Controller Chip fitted to the BBC Micro's motherboard. The filing system was of extremely limited functionality and storage capability, using a flat directory structure. Each filename can be up to seven letters long, plus one letter for the directory in which the file is stored.[1]

The DFS is remarkable in that unlike most filing systems, there was no single vendor or implementation. The original DFS was written by Acorn, who continued to maintain their own codebase, but various disc drive vendors wrote their own implementations. Companies who wrote their own DFS implementation included Cumana, Opus and Watford Electronics. The Watford Electronics implementation is notable for supporting 62 files per disc instead of the usual 31, using a non-standard disc format. Other features in third-party implementations included being able to review free space, and built-in FORMAT and VERIFY commands, which were shipped on a utility disc with the original Acorn DFS.

Acorn followed up their original DFS series with the Acorn 1770 DFS, which used the same disc format as the earlier version but added a set of extra commands and supported the improved WD1770 floppy drive controller chip.
===
The Advanced Disc Filing System (ADFS) is a computing file system particular to the Acorn computer range and RISC OS-based successors. Initially based on the rare Acorn Winchester Filing System, it was renamed to the Advanced Disc Filing System when support for floppy discs was added (utilising a WD1770 floppy disc controller) and on later 32-bit systems a variant of a PC-style floppy controller.[2]

Acorn's original Disc Filing System was limited to 31 files per disk surface, 7 characters per file name and a single character for directory names, a format inherited from the earlier Atom and System 3–5 Eurocard computers. To overcome some of these restrictions Acorn developed ADFS. The most dramatic change was the introduction of a hierarchical directory structure. The filename length increased from 7 to 10 letters and the number of files in a directory expanded to 47. It retained some superficial attributes from DFS; the directory separator continued to be a dot and $ now indicated the hierarchical root of the filesystem. "^" (minus the quotes) was used to refer to the parent directory and "\" was the previously-visited directory.

The BBC Master Compact contained ADFS Version 2.0, which provided the addition of format, verify and backup commands in ROM.[3]
===
A floppy disk, also called a diskette, is a disk storage medium composed of a disk of thin and flexible magnetic storage medium, sealed in a rectangular plastic carrier lined with fabric that removes dust particles. Floppy disks are read and written by a floppy disk drive (FDD).

Floppy disks, initially as 8-inch (200 mm) media and later in 5¼-inch (133 mm) and 3½-inch (90 mm) sizes, were a ubiquitous form of data storage and exchange from the mid-1970s well into the 2000s.[1]

By 2010, computer motherboards were rarely manufactured with floppy drive support; 3½-inch floppy disks can be used with an external USB floppy disk drive, but USB drives for 5¼-inch, 8-inch and non-standard diskettes are rare or non-existent, and those formats must usually be handled by old equipment.

While floppy disk drives still have some limited uses, especially with legacy industrial computer equipment, they have been superseded by data storage methods with much greater capacity, such as USB flash drives, portable external hard disk drives, optical discs, memory cards and computer networks.
==
8-inch floppy disk[edit]

8-inch floppy disk
The first floppy disk was 8 inches in diameter, was protected by a flexible plastic jacket and was a read-only device used by IBM as a way of loading microcode.[16] Read/Write floppy disks and their drives became available in 1972 but it was IBM's 1973 introduction of the 3740 data entry system[17] that began the establishment of floppy disks, called by IBM the "Diskette 1," as an industry standard for information interchange. Early microcomputers used for engineering, business, or word processing often used one or more 8-inch disk drives for removable storage; the CP/M operating system was developed for microcomputers with 8-inch drives.

The family of 8-inch disks and drives increased over time and later versions could store up to 1.2 MB;[18] many microcomputer applications did not need that much capacity on one disk, so a smaller size disk with lower-cost media and drives was feasible. The 5¼-inch inch drive succeeded the 8-inch size in many applications, and developed to about the same storage capacity as the original 8-inch size, using higher-density media and recording techniques.
===
5¼-inch floppy disk[edit]

Uncovered 5¼‑inch disk mechanism with disk inserted. The edge of the disk with the opening for the medium was inserted first, then the lever was turned to close the mechanism and engage the drive motor and heads.
The head gap of an 80‑track high-density (1.2 MB in the MFM format) 5¼‑inch drive (a.k.a. Mini diskette, Mini disk, or Minifloppy) is smaller than that of a 40‑track double-density (360 KB) drive but can format, read and write 40‑track disks well provided the controller supports double stepping or has a switch to do such a process. A blank 40‑track disk formatted and written on an 80‑track drive can be taken to its native drive without problems, and a disk formatted on a 40‑track drive can be used on an 80‑track drive. Disks written on a 40‑track drive and then updated on an 80 track drive become unreadable on any 40‑track drives due to track width incompatibility.

Single sided disks were coated on both sides, despite the availability of more expensive double sided disks. The reason usually given for the higher cost was that double sided disks were certified error-free on both sides of the media. Architectural differences among computer platforms negated this claim, however, with RadioShack TRS-80 Model I computers using one side and the Apple II machines the other. Double-sided disks could be used in drives for single-sided disks, one side at a time, by turning them over (flippy disks); more expensive dual-head drives which could read both sides without turning over were later produced, and eventually became used universally.
==
3½-inch floppy disk[edit]

Internal parts of a 3½‑inch floppy disk.
1) A hole that indicates a high-capacity disk.
2) The hub that engages with the drive motor.
3) A shutter that protects the surface when removed from the drive.
4) The plastic housing.
5) A polyester sheet reducing friction against the disk media as it rotates within the housing.
6) The magnetic coated plastic disk.
7) A schematic representation of one sector of data on the disk; the tracks and sectors are not visible on actual disks.
8) The write protection tab (unlabeled) is upper left.

A 3½-inch floppy disk drive.
In the early 1980s, a number of manufacturers introduced smaller floppy drives and media in various formats. A consortium of 21 companies eventually settled on a 3½-inch floppy disk (actually 90 mm wide) a.k.a. Micro diskette, Micro disk, or Micro floppy, similar to a Sony design, but improved to support both single-sided and double-sided media, with formatted capacities generally of 360 KB and 720 KB respectively. Single-sided drives shipped in 1983,[19] and double sided in 1984. What became the most common format, the double-sided, high-density (HD) 1.44 MB disk drive, shipped in 1986.

The first Macintosh computers used single-sided 3½-inch inch floppy disks, but with 400 KB formatted capacity. These were followed in 1986 by double-sided 800 KB floppies. The higher capacity was achieved at the same recording density by varying the disk rotation speed with arm position so that the linear speed of the head was closer to constant. Later Macs could also read and write 1.44 MB HD disks in PC format with fixed rotation speed.

All 3½-inch disks have a rectangular hole in one corner which, if obstructed, write-enabled the disk. The HD 1.44 MB disks have a second, unobstructed hole in the opposite corner which identifies them as being of that capacity.

In IBM-compatible PCs, the three densities of 3½-inch floppy disks are backwards-compatible: higher density drives can read, write and format lower density media. It is physically possible to format a disk at the wrong density, although the resulting disk will not work properly. Fresh disks manufactured as high density can theoretically be formatted at double density only if no information has been written on the disk in high density, or the disk has been thoroughly demagnetized with a bulk eraser, as the magnetic strength of a high density record is stronger and overrides lower density, remaining on the disk and causing problems.

Writing at different densities than disks were intended for, sometimes by altering or drilling holes, was possible but deprecated. The holes on the right side of a 3½‑inch disk can be altered as to make some disk drives and operating systems treat the disk as one of higher or lower density, for bidirectional compatibility or economical reasons.[clarification needed][20][21] Some computers, such as the PS/2 and Acorn Archimedes, ignored these holes altogether.[22]

It is possible to make a 3½-inch floppy disk drive be recognized by a system as a 5¼‑inch 360 KB or 1200 KB drive, and to read and write disks with the same number of tracks and sectors as those disks; this had some application in data exchange with obsolete CP/M systems.[citation needed]
==
A hard disk drive (HDD), hard disk, hard drive or fixed disk[b] is a data storage device used for storing and retrieving digital information using one or more rigid ("hard") rapidly rotating disks (platters) coated with magnetic material. The platters are paired with magnetic heads arranged on a moving actuator arm, which read and write data to the platter surfaces.[2] Data is accessed in a random-access manner, meaning that individual blocks of data can be stored or retrieved in any order rather than sequentially. An HDD retains its data even when powered off.

Introduced by IBM in 1956,[3] HDDs became the dominant secondary storage device for general-purpose computers by the early 1960s. Continuously improved, HDDs have maintained this position into the modern era of servers and personal computers. More than 200 companies have produced HDD units, though most current units are manufactured by Seagate, Toshiba and Western Digital. Worldwide disk storage revenues were US $32 billion in 2013, down 3% from 2012.[4]

The primary characteristics of an HDD are its capacity and performance. Capacity is specified in unit prefixes corresponding to powers of 1000: a 1-terabyte (TB) drive has a capacity of 1,000 gigabytes (GB; where 1 gigabyte = 1 billion bytes). Typically, some of an HDD's capacity is unavailable to the user because it is used by the file system and the computer operating system, and possibly inbuilt redundancy for error correction and recovery. Performance is specified by the time required to move the heads to a track or cylinder (average access time) plus the time it takes for the desired sector to move under the head (average latency, which is a function of the physical rotational speed in revolutions per minute), and finally the speed at which the data is transmitted (data rate).

The two most common form factors for modern HDDs are 3.5-inch, for desktop computers, and 2.5-inch, primarily for laptops. HDDs are connected to systems by standard interface cables such as SATA (Serial ATA), USB or SAS (Serial attached SCSI) cables.

As of 2015, the primary competing technology for secondary storage is flash memory in the form of solid-state drives (SSDs), which have higher data transfer rates and significantly lower latency and access times, but HDDs remain the dominant medium for secondary storage due to advantages in price per unit of storage and recording capacity.[5][6] However, SSDs are replacing HDDs where speed, power consumption and durability are more important considerations.[7][8]
===
Small Computer System Interface (SCSI, /ˈskʌzi/ skuz-ee)[1] is a set of standards for physically connecting and transferring data between computers and peripheral devices. The SCSI standards define commands, protocols and electrical and optical interfaces. SCSI is most commonly used for hard disk drives and tape drives, but it can connect a wide range of other devices, including scanners and CD drives, although not all controllers can handle all devices. The SCSI standard defines command sets for specific peripheral device types; the presence of "unknown" as one of these types means that in theory it can be used as an interface to almost any device, but the standard is highly pragmatic and addressed toward commercial requirements.
===
A solid-state drive (SSD) (also known as a solid-state disk[1][2][3] though it contains no actual disk, nor a drive motor to spin a disk) is a solid-state storage device that uses integrated circuit assemblies as memory to store data persistently. SSD technology primarily uses electronic interfaces compatible with traditional block input/output (I/O) hard disk drives, which permit simple replacements in common applications.[4] Additionally, new I/O interfaces, like SATA Express, have been designed to address specific requirements of the SSD technology.

SSDs have no moving (mechanical) components. This distinguishes them from traditional electromechanical magnetic disks such as hard disk drives (HDDs) or floppy disks, which contain spinning disks and movable read/write heads.[5] Compared with electromechanical disks, SSDs are typically more resistant to physical shock, run silently, have lower access time, and less latency.[6] However, while the price of SSDs has continued to decline over time,[7] consumer-grade SSDs are still roughly six to seven times more expensive per unit of storage than consumer-grade HDDs.

As of 2014, most SSDs use NAND-based flash memory, which retains data without power. For applications requiring fast access, but not necessarily data persistence after power loss, SSDs may be constructed from random-access memory (RAM). Such devices may employ separate power sources, such as batteries, to maintain data after power loss.[4]

Hybrid drives or solid-state hybrid drives (SSHDs) combine the features of SSDs and HDDs in the same unit, containing a large hard disk drive and an SSD cache to improve performance of frequently accessed data.[8][9][10]
===
A light pen is a computer input device in the form of a light-sensitive wand used in conjunction with a computer's CRT display.

It allows the user to point to displayed objects or draw on the screen in a similar way to a touchscreen but with greater positional accuracy. A light pen can work with any CRT-based display and other display technologies, but its ability to be used with LCDs was unclear (though Toshiba and Hitachi displayed a similar idea at the "Display 2006" show in Japan[1]).

A light pen detects a change of brightness of nearby screen pixels when scanned by cathode ray tube electron beam and communicates the timing of this event to the computer. Since a CRT scans the entire screen one pixel at a time, the computer can keep track of the expected time of scanning various locations on screen by the beam and infer the pen's position from the latest timestamp.

The first light pen was created around 1955 as part of the Whirlwind project at MIT.[2][3]

During the 1960s light pens were common on graphics terminals such as the IBM 2250, and were also available for the IBM 3270 text-only terminal.

The light pen was used in the early 1980s. It was notable for its use in the Fairlight CMI, and the BBC Micro. IBM PC compatible CGA, HGC and some EGA graphics cards featured a connector for a light pen as well. Even some consumer products were given light pens, such as the Thomson MO5 computer family as well as the Atari 8-bit and Commodore 8-bit home computers.

Because the user was required to hold his/her arm in front of the screen for long periods of time or to use a desk that tilts the monitor, the light pen fell out of use as a general purpose input device.[citation needed]
===
A trackball is a pointing device consisting of a ball held by a socket containing sensors to detect a rotation of the ball about two axes—like an upside-down mouse with an exposed protruding ball. The user rolls the ball with the thumb, fingers, or the palm of the hand to move a pointer.

Compared with a mouse, a trackball has no limits on effective travel; at times, a mouse can reach an edge of its working area while the operator still wishes to move the screen pointer farther. With a trackball, the operator just continues rolling, whereas a mouse would have to be lifted and re-positioned. Some trackballs have notably low friction, as well as being made of dense material such as glass, so they can be spun to make them coast. The trackball's buttons may be situated to that of a mouse or to a unique style that suits the user.

Large trackballs are common on CAD workstations for easy precision. Before the advent of the touchpad, small trackballs were common on portable computers, where there may be no desk space on which to run a mouse. Some small thumbballs clip onto the side of the keyboard and have integral buttons with the same function as mouse buttons.

The trackball was invented as part of a post-World War II-era radar plotting system named Comprehensive Display System (CDS) by Ralph Benjamin when working for the British Royal Navy Scientific Service.[1][2] Benjamin's project used analog computers to calculate the future position of target aircraft based on several initial input points provided by a user with a joystick. Benjamin felt that a more elegant input device was needed and invented a ball tracker[1][2] system called the roller ball[1] for this purpose in 1946.[1][2] The device was patented in 1947,[1] but only a prototype using a metal ball rolling on two rubber-coated wheels was ever built[2] and the device was kept as a military secret.[2]
===
A graphics tablet or digitizer is a computer input device that enables a user to hand-draw images, animations and graphics, similar to the way a person draws images with a pencil and paper. These tablets may also be used to capture data or handwritten signatures. It can also be used to trace an image from a piece of paper which is taped or otherwise secured to the surface. Capturing data in this way, by tracing or entering the corners of linear poly-lines or shapes, is called digitizing.

The device consists of a flat surface upon which the user may "draw" or trace an image using an attached stylus, a pen-like drawing apparatus. The image is displayed on the computer monitor, although some graphics tablets also have a screen.

Some tablets are intended as a replacement for the mouse as the primary pointing and navigation device for desktop computers.

The first electronic handwriting device was the Telautograph, patented by Elisha Gray in 1888.[1] Elisha Gray is best known as a contemporaneous inventor of the telephone to Alexander Graham Bell.

The first graphics tablet resembling contemporary tablets and used for handwriting recognition by a computer was the Stylator in 1957.[2] Better known (and often misstated as the first digitizer tablet) is the RAND Tablet[3] also known as the Grafacon[4] (for Graphic Converter), introduced in 1964. The RAND Tablet employed a grid of wires under the surface of the pad that encoded horizontal and vertical coordinates in a small magnetic signal. The stylus would receive the magnetic signal, which could then be decoded back as coordinate information.

The acoustic tablet, or spark tablet, used a stylus that generated clicks with a spark plug. The clicks were then triangulated by a series of microphones to locate the pen in space.[5] The system was fairly complex and expensive, and the sensors were susceptible to interference by external noise.

Digitizers were popularized in the mid 1970s and early 1980s by the commercial success of the ID (Intelligent Digitizer) and BitPad manufactured by the Summagraphics Corp. These digitizers were used as the input device for many high-end CAD (Computer Aided Design) systems as well as bundled with PCs and PC-based CAD software like AutoCAD.

Summagraphics also made an OEM version of its BitPad which was sold by Apple Computer as the Apple Graphics Tablet accessory to their Apple II. These tablets used a magnetostriction technology which used wires made of a special alloy stretched over a solid substrate to accurately locate the tip of a stylus or the center of a digitizer cursor on the surface of the tablet. This technology also allowed Proximity or "Z" axis measurement.

The first home computer graphics tablet was the KoalaPad. Though originally designed for the Apple II, the Koala eventually broadened its applicability to practically all home computers with graphics support, examples of which include the TRS-80 Color Computer, Commodore 64, and Atari 8-bit family. Competing tablets were eventually produced; the tablets produced by Atari were generally considered to be of high quality.

In 1981, musician Todd Rundgren created the first color graphics tablet software for personal computers, which was licensed to Apple as the Utopia Graphics Tablet System.[6]

In the 1980s, several vendors of graphics tablets began to include additional functions, such as handwriting recognition and on-tablet menus.[7][8]
===
A webcam is a video camera that feeds or streams its image in real time to or through a computer to computer network. When "captured" by the computer, the video stream may be saved, viewed or sent on to other networks via systems such as the internet, and email as an attachment. When sent to a remote location, the video stream may be saved, viewed or on sent there. Unlike an IP camera (which connects using Ethernet or Wi-Fi), a webcam is generally connected by a USB cable, or similar cable, or built into computer hardware, such as laptops.

The term 'webcam' (a clipped compound) may also be used in its original sense of a video camera connected to the Web continuously for an indefinite time, rather than for a particular session, generally supplying a view for anyone who visits its web page over the Internet. Some of them, for example, those used as online traffic cameras, are expensive, rugged professional video cameras.
===
The plotter is a computer printer for printing vector graphics. In the past, plotters were used in applications such as computer-aided design, though they have generally been replaced with wide-format conventional printers. A plotter gives a hard copy of the output. It draws pictures on a paper using a pen. Plotters are used to print designs of ships and machines, plans for buildings and so on.

Pen plotters print by moving a pen or other instrument across the surface of a piece of paper. This means that plotters are vector graphics devices, rather than raster graphics as with other printers. Pen plotters can draw complex line art, including text, but do so slowly because of the mechanical movement of the pens. They are often incapable of efficiently creating a solid region of color, but can hatch an area by drawing a number of close, regular lines.

Plotters offered the fastest way to efficiently produce very large drawings or color high-resolution vector-based artwork when computer memory was very expensive and processor power was very limited, and other types of printers had limited graphic output capabilities.

Pen plotters have essentially become obsolete, and have been replaced by large-format inkjet printers and LED toner based printers. Such devices may still understand vector languages originally designed for plotter use, because in many uses, they offer a more efficient alternative to raster data.

The Z64, the image you see, is a flatbed drawing machine of high precision.[1]

Electrostatic plotters[edit]
Electrostatic plotters used a dry toner transfer process similar to that in many photocopiers. They were faster than pen plotters and were available in large formats, suitable for reproducing engineering drawings. The quality of image was often not as good as contemporary pen plotters. Electrostatic plotters were made in both flat-bed and drum types.

Cutting plotters[edit]
Cutting plotters use knives to cut into a piece of material (such as paper, mylar or vinyl) that is lying on the flat surface area of the plotter. It is achieved because the cutting plotter is connected to a computer, which is equipped with specialized cutting design or drawing computer software programs. Those computer software programs are responsible for sending the necessary cutting dimensions or designs in order to command the cutting knife to produce the correct project cutting needs.[2]

In recent years the use of cutting plotters (generally called die-cut machines) has become popular with home enthusiasts of paper crafts such as cardmaking and scrapbooking. Such tools allow desired card shapes to be cut out very precisely, and repeated perfectly identically.

===
Computer-aided manufacturing (CAM) is the use of computer software to control machine tools and related machinery in the manufacturing of workpieces.[1][2][3][4][5] This is not the only definition for CAM, but it is the most common;[1] CAM may also refer to the use of a computer to assist in all operations of a manufacturing plant, including planning, management, transportation and storage.[6][7] Its primary purpose is to create a faster production process and components and tooling with more precise dimensions and material consistency, which in some cases, uses only the required amount of raw material (thus minimizing waste), while simultaneously reducing energy consumption.[citation needed] CAM is now a system used in schools and lower educational purposes. CAM is a subsequent computer-aided process after computer-aided design (CAD) and sometimes computer-aided engineering (CAE), as the model generated in CAD and verified in CAE can be input into CAM software, which then controls the machine tool.

Traditionally, CAM has been considered as a numerical control (NC) programming tool, where in two-dimensional (2-D) or three-dimensional (3-D) models of components generated in CADAs with other “Computer-Aided” technologies, CAM does not eliminate the need for skilled professionals such as manufacturing engineers, NC programmers, or machinists. CAM, in fact, leverages both the value of the most skilled manufacturing professionals through advanced productivity tools, while building the skills of new professionals through visualization, simulation and optimization tools.

===
Computer-aided engineering (CAE) is the broad usage of computer software to aid in engineering analysis tasks. It includes Finite Element Analysis (FEA), Computational Fluid Dynamics (CFD), Multibody dynamics (MBD), and optimization.

Software tools that have been developed to support these activities are considered CAE tools. CAE tools are being used, for example, to analyze the robustness and performance of components and assemblies. The term encompasses simulation, validation, and optimization of products and manufacturing tools. In the future, CAE systems will be major providers of information to help support design teams in decision making. Computer-aided engineering is used in many fields such as automotive, aviation, space, and shipbuilding industries.[1]

In regard to information networks, CAE systems are individually considered a single node on a total information network and each node may interact with other nodes on the network.

CAE systems can provide support to businesses. This is achieved by the use of reference architectures and their ability to place information views on the business process. Reference architecture is the basis from which information model, especially product and manufacturing models.

The term CAE has also been used by some in the past to describe the use of computer technology within engineering in a broader sense than just engineering analysis. It was in this context that the term was coined by Jason Lemon, founder of SDRC in the late 1970s. This definition is however better known today by the terms CAx and PLM.[citation needed]


===
Numerical control (NC) is the automation of machine tools that are operated by precisely programmed commands encoded on a storage medium, as opposed to controlled manually via hand wheels or levers, or mechanically automated via cams alone. Most NC today is computer (or computerized) numerical control (CNC),[1] in which computers play an integral part of the control.

In modern CNC systems, end-to-end component design is highly automated using computer-aided design (CAD) and computer-aided manufacturing (CAM) programs. The programs produce a computer file that is interpreted to extract the commands needed to operate a particular machine via a post processor, and then loaded into the CNC machines for production. Since any particular component might require the use of a number of different tools – drills, saws, etc., modern machines often combine multiple tools into a single "cell". In other installations, a number of different machines are used with an external controller and human or robotic operators that move the component from machine to machine. In either case, the series of steps needed to produce any part is highly automated and produces a part that 

===
Direct numerical control (DNC), also known as distributed numerical control (also DNC), is a common manufacturing term for networking CNC machine tools. On some CNC machine controllers, the available memory is too small to contain the machining program (for example machining complex surfaces), so in this case the program is stored in a separate computer and sent directly to the machine, one block at a time. If the computer is connected to a number of machines it can distribute programs to different machines as required. Usually, the manufacturer of the control provides suitable DNC software. However, if this provision is not possible, some software companies provide DNC applications that fulfill the purpose. DNC networking or DNC communication is always required when CAM programs are to run on some CNC machine control.

Wireless DNC is also used in place of hard-wired versions. Controls of this type are very widely used in industries with significant sheet metal fabrication, such as the automotive, appliance, and aerospace industries.

===

British Aerospace plc (BAe) was a British aircraft, munitions and defence-systems manufacturer. Its head office was at Warwick House in the Farnborough Aerospace Centre in Farnborough, Hampshire.[1] In 1999 it purchased Marconi Electronic Systems, the defence electronics and naval shipbuilding subsidiary of the General Electric Company plc, to form BAE Systems.

===

A video card (also called a video adapter, display card, graphics card, graphics board, display adapter, graphics adapter or frame buffer[1]) is an expansion card which generates a feed of output images to a display (such as a computer monitor). Frequently, these are advertised as discrete or dedicated graphics cards, emphasizing the distinction between these and integrated graphics. Within the industry, video cards are sometimes called graphics add-in-boards, abbreviated as AIBs,[2] with the word "graphics" usually omitted.

Standards such as MDA, CGA, HGC, Tandy, PGC, EGA, VGA, MCGA, 8514 or XGA were introduced from 1982 to 1990 and supported by a variety of hardware manufacturers.

Virtually all current video cards are built with either AMD-sourced or Nvidia-sourced graphics chips.[2] Most video cards offer various functions such as accelerated rendering of 3D scenes and 2D graphics, MPEG-2/MPEG-4 decoding, TV output, or the ability to connect multiple monitors (multi-monitor).
===
The Monochrome Display Adapter (MDA, also MDA card, Monochrome Display and Printer Adapter, MDPA) introduced in 1981 was IBM's standard video display card and computer display standard for the PC. The MDA did not have any pixel-addressable graphics modes. It had only a single monochrome text mode (PC video mode 7), which could display 80 columns by 25 lines of high resolution text characters or symbols useful for drawing forms.

The standard IBM MDA card was equipped with four kilobytes (kB) of video memory. The MDA's high character resolution (sharpness) was a feature meant to facilitate business and wordprocessing use: Each character was rendered in a box of 9×14 pixels, of which 8×14 made out the character itself (the other pixels being used for space between character columns and lines). Some characters, such as the lowercase "m", were rendered eight pixels across.

The MDA featured the following character display attributes: invisible, underline, normal, bright (bold), reverse video, and blinking; some of these attributes could be combined, so that e.g., bright, underlined text could be produced.[1]

The theoretical total screen display resolution of the MDA was 720×350 pixels. This number is arrived at through calculating character width (nine pixels) by columns of text (80) and character height (14 pixels) by rows of text (25). However, the MDA again could not address individual pixels; it could only work in text mode, limiting its choice of display patterns to 256 characters. Its character set is known as code page 437. The character patterns were stored in ROM on the card, and so could not be changed by software. The only way to simulate "graphical" screen content was through ASCII art.

Because of the lack of pixel-addressable graphics, MDA owners could not play most graphics-based games. At least one game, IBM's One Hundred And One Monochrome Mazes ("Amazing fun for the whole family"), required MDA.[2] Code page 437 included the standard 127 ASCII characters but also another 127 characters like the aforementioned characters for drawing forms. Some of these shapes would later show up in Unicode as box-drawing characters. The characters were also used in early PC games such as early BBS door games, or games like Castle Adventure by Kevin Bales.

IBM's original MDA included a parallel printer port (hence its original name of "Monochrome Display and Printer Adapter"), thus avoiding the need for a separate parallel interface on computers fitted with an MDA.
===
The Color Graphics Adapter (CGA), originally also called the Color/Graphics Adapter or IBM Color/Graphics Monitor Adapter,[1] introduced in 1981, was IBM's first graphics card and first color display card for the IBM PC. For this reason, it also became that computer's first color computer display standard.

The standard IBM CGA graphics card was equipped with 16 kilobytes of video memory and could be connected either to a dedicated direct-drive CRT monitor using a 4-bit digital (TTL) "RGBI"[2] interface, such as the IBM 5153 color display, or to an NTSC-compatible television or composite video monitor via an RCA connector.[3] The RCA connector provided only baseband video, so to connect the CGA card to a standard television set required a separate RF modulator.[4]

Built around the Motorola MC6845 display controller, the CGA card featured several graphics and text modes. The highest display resolution of any mode was 640×200, and the highest color depth supported was 4-bit (16 colors).
===
The Hercules Graphics Card (HGC) was a computer graphics controller made by Hercules Computer Technology, Inc. that combined IBM's text-only MDA display standard with a bitmapped graphics mode. This allowed the HGC to offer both high quality text and graphics from a single card. The HGC was very popular, and became a widely supported de facto display standard on IBM PC compatibles connected to a monochrome monitor. The HGC standard was used long after more technically capable systems had entered the market, especially on dual-monitor setups.
===
Tandy Graphics Adapter (TGA) is a computer display standard for an IBM PC compatible video adapter that improved on IBM's Color Graphics Adapter (CGA) technology. Whereas CGA could display only four colors at a time at a screen resolution of 320×200 pixels, a TGA adapter could display up to 16 colors.

IBM premiered the new video adapter in the PCjr home computer it announced in 1983. At that time, the company called the adapter the Video Gate Array (not to be confused with the later Video Graphics Array standard), or CGA Plus. The adapter incorporated the Motorola 6845, an integrated circuit for controlling video.

The PCjr was not popular, but the Tandy 1000 family of home computers were compatible with its video standard and Texas Instruments SN76489-based enhanced sound, and sold well. With built-in joystick ports the 1000 was the best platform for IBM PC-compatible games before the VGA era, and the combination of its graphics and sound became known as "Tandy compatible";[1][2] 28 of 66 games that Computer Gaming World tested in 1989 supported Tandy graphics.[3] The later Tandy 1000 SL and TL models were equipped with an enhanced version of the CGA Plus adapter, capable of displaying 16 colors at an improved resolution of 640x200.[4]
===
Professional Graphics Controller (PGC, often called Professional Graphics Adapter and sometimes Professional Graphics Array) was a graphics card manufactured by IBM for PCs.[1] It consisted of three interconnected PCBs, and contained its own processor and memory. The PGC was, at the time of its release, the most advanced graphics card for the IBM XT and aimed for tasks such as CAD.[2]

Introduced in 1984,[3] the Professional Graphics Controller offered a maximum resolution of 640×480 with 256 colors at a refresh rate of 60 Hertz—a higher resolution and color depth than EGA and VGA supported. This mode is not BIOS-supported. It was intended for the computer-aided design market and included 320 kB of display RAM and an on-board Intel 8088 microprocessor. The 8088 was placed directly on the card to permit rapid updates of video memory. Other cards forced the PC's CPU to write to video memory through a slower ISA bus. While never widespread in consumer-class personal computers, its US $4,290 list price compared favorably to US$50,000 dedicated CAD workstations of the time. It was discontinued in 1987 with the arrival of VGA and 8514.
===
The Enhanced Graphics Adapter (EGA) is an IBM PC computer display standard specification which is between CGA and VGA in terms of color and space resolution. Introduced in October 1984[2][3] by IBM shortly after (but not exclusively for) its new PC/AT, EGA produces a display of 16 simultaneous colors from a palette of 64 at a resolution of up to 640×350 pixels, a bit more than Tandy which can do up to 16 simultaneous colors from the same palette at a resolution of up to 320×200 but only 4 colors at 640×200. The EGA card includes a 16 KB ROM to extend the system BIOS for additional graphics functions, and includes a custom CRT controller that has a backward compatibility mode with Motorola MC6845 chip.[4]

Each of the 16 colors can be assigned a unique RGB color code via a palette mechanism in the 640×350 high-resolution mode; the 64 palette colors are a balanced RGB color set comprising all possible combinations of two bits per pixel for red, green and blue. EGA also includes full 16-color versions of the CGA 640×200 and 320×200 graphics modes; only the 16 CGA/RGBI colors are available in these modes[citation needed]. EGA 4-bit (16 colors) graphic modes are also notable for a sophisticated use of bit planes and mask registers[5] together with CPU bitwise operations,[6] which constitutes an early graphics accelerator inherited by VGA and numerous compatible hardware.

EGA is dual-sync; it scans at 23 kHz when 350-line modes are used and 15 kHz when 200-line modes are used. The original CGA modes are also present, though EGA is not 100% hardware compatible with CGA. EGA can drive an MDA monitor by a special setting of switches on the board; only 640×350 high-resolution monochrome graphics and the standard MDA text mode are available in this mode.

EGA cards use the PC ISA bus and were available starting in 8-bit versions. The original IBM EGA card had 64k of onboard RAM and required a daughterboard to add an additional 64k (cards with 64k are limited to 4 colors when 640x350 mode is used). All third-party cards came with 128k already installed and some even 256k, allowing multiple graphics pages. A few third-party EGA clones (notably the ATI Technologies and Paradise boards) feature a range of extended graphics modes (e.g., 640×400, 640×480 and 720×540), as well as automatic monitor type detection, and sometimes also a special 400-line interlace mode for use on CGA monitors.

The EGA standard was made obsolete by the introduction in 1987 of MCGA with the PS/2 computer line[7] and VGA.

Shortly before the introduction of VGA, Genoa Systems introduced a half-size graphics card built around a proprietary chip set, which they called Super EGA (later cards supporting an extended version of the VGA were similarly named Super VGA).[8]
===
Video Graphics Array (VGA) refers specifically to the display hardware first introduced with the IBM PS/2 line of computers in 1987,[1] but through its widespread adoption has also come to mean either an Amplitude Modulated computer display standard, the 15-pin D-subminiature VGA connector or the 640x480 resolution itself.

VGA was the last IBM graphics standard to which the majority of PC clone manufacturers conformed, making it the lowest common denominator that virtually all post-1990 PC graphics hardware can be expected to implement. It was officially followed by IBM's Extended Graphics Array (XGA) standard, but was effectively superseded by numerous slightly different extensions to VGA made by clone manufacturers, collectively known as Super VGA.

Today, the VGA analog interface is used for high definition video, including resolutions of 1080p and higher. While the transmission bandwidth of VGA is high enough to support even higher resolution playback, there can be picture quality degradation depending on cable quality and length. How discernible this degradation is depends on the individual's eyesight and the display, though it is more noticeable when switching to and from digital inputs like HDMI or DVI.
===
The Multi-Color Graphics Array or MCGA was a video subsystem built into the motherboard of the IBM PS/2 Model 30, introduced on April 2, 1987, and Model 25, introduced later on August 11; no standalone MCGA cards were ever made.[1]

The MCGA supported all CGA display modes plus 640×480 monochrome at a refresh rate of 60 Hz, and 320×200 with 256 colors (out of a palette of 262,144) at 70 Hz. The display adapter used a DE-15 connector. The MDA monochrome text mode was not supported.

MCGA was similar to VGA in that it had a 256-color mode (the 256-color mode in VGA was sometimes referred to as MCGA) and used 15-pin analog connectors. The PS/2 chipset's limited abilities prevented EGA compatibility and high-resolution multi-color VGA display modes.

The tenure of MCGA was brief; the PS/2 Model 25 and Model 30 were discontinued by 1992, and no manufacturer produced a clone of this display adapter except for Epson Equity Ie, since the VGA standard introduced at the same time was considered superior.[2]
===
IBM 8514 is an IBM graphics computer display standard supporting a display resolution of 1024x768 pixels with 256 colors at 43.5 Hz (interlaced), or 640x480 at 60 Hz (non-interlaced).[1] 8514 usually refers to the display controller hardware (such as the 8514/A display adapter.) However, IBM sold the companion CRT monitor (for use with the 8514/A) which carries the same designation, 8514.

8514 used a standardised programming interface called the "Adapter Interface" or AI. This interface is also used by XGA, IBM Image Adapter/A, and clones of the 8514/A and XGA such as the ATI Technologies Mach 32 and IIT AGX. The interface allows computer software to offload common 2D-drawing operations (line-draw, color-fill, BITBLT) onto the 8514 hardware. This freed the host CPU for other tasks, and greatly improved the speed of redrawing a graphics visual (such as a pie-chart or CAD-illustration).
===
Super Video Graphics Array or Ultra Video Graphics Array,[1] almost always abbreviated to Super VGA, Ultra VGA or just SVGA or UVGA is a broad term that covers a wide range of computer display standards.[2]


SVGA (4:3) compared with the other display standards.

15-pin D-sub port
Originally, it was an extension to the VGA standard first released by IBM in 1987. Unlike VGA—a purely IBM-defined standard—Super VGA was never formally defined. The closest to an "official" definition was in the VBE extensions defined by the Video Electronics Standards Association (VESA), an open consortium set up to promote interoperability and define standards. In this document, there was simply a footnote stating that "The term 'Super VGA' is used in this document for a graphics display controller implementing any superset of the standard IBM VGA display adapter."[3] When used as a resolution specification, in contrast to VGA or XGA for example, the term SVGA normally refers to a resolution of 800x600 pixels.

Though Super VGA cards appeared in the same year as VGA (1987),[citation needed] it wasn't until 1989 that a standard for programming Super VGA modes was defined by VESA. In that first version, it defined support for (but did not require) a maximum resolution of 800x600 4-bit pixels. Each pixel could therefore be any of 16 different colors. It was quickly extended to 1024x768 8-bit pixels, and well beyond that in the following years.[citation needed]

Although the number of colors is defined in the VBE specification, this is irrelevant when referring to Super VGA monitors as (in contrast to the old CGA and EGA standards) the interface between the video card and the VGA or Super VGA monitor uses simple analog voltages to indicate the desired color. In consequence, so far as the monitor is concerned, there is no theoretical limit to the number of different colors that can be displayed. This applies to any VGA or Super VGA monitor.

While the output of a VGA or Super VGA video card is analog, the internal calculations the card performs in order to arrive at these output voltages are entirely digital. To increase the number of colors a Super VGA display system can reproduce, no change at all is needed for the monitor, but the video card needs to handle much larger numbers and may well need to be redesigned from scratch. Even so, the leading graphics chip vendors were producing parts for high-color video cards within just a few months of Super VGA's introduction.

On paper, the original Super VGA was to be succeeded by Super XGA[citation needed], but in practice the industry soon abandoned the attempt to provide a unique name for each higher display standard, and almost all display systems made between the late 1990s and the early 2000s are classed as Super VGA.

Monitor manufacturers sometimes advertise their products as XGA or Super XGA. In practice this means little, since all Super VGA monitors manufactured since the later 1990s have been capable of at least XGA and usually considerably higher performance.

SVGA uses a VGA connector, the same DE-15 (a.k.a. HD-15) as the original standard.
===
XGA (1024x768)[edit]
XGA, the Extended Graphics Array, is an IBM display standard introduced in 1990. Later it became the most common appellation of the 1024x768 pixels display resolution, but the official definition is broader than that. It was not a new and improved replacement for Super VGA, but rather became one particular subset of the broad range of capabilities covered under the "Super VGA" umbrella.

The initial version of XGA (and its predecessor, the IBM 8514) expanded upon IBM's older VGA by adding support for four new screen modes (three, for the 8514), including one new resolution:[11]

640x480 pixels in direct 16 bits-per-pixel (65,536 color) RGB hi-color (XGA only, with 1 MB video memory option) and 8 bpp (256 color) palette-indexed mode.
1024x768 pixels with a 16- or 256-color (4 or 8 bpp) palette, using a low frequency interlaced refresh rate (again, the higher 8 bpp mode required 1 MB VRAM[12]).
Like the 8514, XGA offered fixed function hardware acceleration to offload processing of 2D drawing tasks. Both adaptors allowed offloading of line-draw, bitmap-copy (bitblt), and color-fill operations from the host CPU. XGA's acceleration was faster than 8514's, and more comprehensive, supporting more drawing primitives, the VGA-res hi-color mode, versatile "brush" and "mask" modes, system memory addressing functions, and a single simple hardware sprite typically used to providing a low CPU load mouse pointer. It was also capable of wholly independent function, as it incorporated support for all existing VGA functions and modes – the 8514 itself was a simpler add-on adaptor that required a separate VGA to be present. It should be noted that, as they were designed for use with IBM's own range of fixed-frequency monitors, neither adaptor offered support for 800x600 SVGA modes.

XGA-2 added a 24-bit DAC, but this was used only to extend the available master palette in 256-color mode, e.g. to allow true 256-greyscale output instead of the 64 grey levels previously available; there was still no direct Truecolor mode despite the adaptor featuring enough default onboard VRAM (1 MB) to support it. Other improvements included provision of the previously missing 800x600 resolution (using an SVGA or multisync monitor) in up to 65,536 colors, faster screen refresh rates in all modes (including non-interlace, flicker-free output for 1024x768), and improved accelerator performance and versatility.

IBM licensed the XGA technology and architecture to certain third party hardware developers, and its characteristic modes (although not necessarily the accelerator functions, nor the MCA data-bus interface) were aped by many others. These accelerators typically did not suffer from the same limitations on available resolutions and refresh rate, and featured other now-standard modes like 800x600 (and 1280x1024) at various color depths (up to 24 bpp Truecolor) and interlaced, non-interlaced and flicker-free refresh rates even before the release of the XGA-2.

All standard XGA modes have a 4:3 aspect ratio with square pixels, although this does not hold for certain standard VGA and third-party extended modes (640x400, 1280x1024).

XGA should not be confused with EVGA (Extended Video Graphics Array), a contemporaneous VESA standard that also has 1024x768 pixels. It should also not be confused with the Expanded Graphics Adapter, a peripheral for the IBM 3270 PC which can also be referred to as XGA.[13]
===
